#!/usr/bin/env python3
"""
An√°lisis detallado de resultados y estrategias para alcanzar recompensa -30
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def cargar_y_analizar_resultados():
    """Carga y analiza los resultados del experimento"""
    
    print("="*80)
    print("üìä AN√ÅLISIS DETALLADO DE RESULTADOS - PROYECTO FLAN")
    print("="*80)
    
    try:
        with open('flan_results_10k.json', 'r') as f:
            results = json.load(f)
        
        # Extraer datos
        scheme_results = results['Media']
        qlearning_data = scheme_results['qlearning']
        stochastic_data = scheme_results['stochastic']
        
        print("‚úÖ Resultados cargados exitosamente")
        
        return qlearning_data, stochastic_data, results
        
    except FileNotFoundError:
        print("‚ùå Error: No se encontr√≥ el archivo flan_results_10k.json")
        return None, None, None

def analizar_rendimiento_actual(qlearning_data, stochastic_data):
    """Analiza el rendimiento actual en detalle"""
    
    print("\n" + "="*60)
    print("üîç AN√ÅLISIS DE RENDIMIENTO ACTUAL")
    print("="*60)
    
    # Datos Q-Learning
    ql_rewards = np.array(qlearning_data['evaluation']['total_rewards'])
    ql_best_score = qlearning_data['best_score']
    
    # Datos Stochastic Q-Learning  
    stoch_rewards = np.array(stochastic_data['evaluation']['total_rewards'])
    stoch_best_score = stochastic_data['best_score']
    
    print(f"ü§ñ Q-LEARNING EST√ÅNDAR:")
    print(f"   ‚Ä¢ Score promedio b√∫squeda: {ql_best_score:.2f}")
    print(f"   ‚Ä¢ Recompensa evaluaci√≥n: {np.mean(ql_rewards):.2f} ¬± {np.std(ql_rewards):.2f}")
    print(f"   ‚Ä¢ Rango: [{np.min(ql_rewards):.2f}, {np.max(ql_rewards):.2f}]")
    print(f"   ‚Ä¢ Mejor episodio: {np.max(ql_rewards):.2f}")
    print(f"   ‚Ä¢ Percentil 90: {np.percentile(ql_rewards, 90):.2f}")
    
    print(f"\nüé≤ STOCHASTIC Q-LEARNING:")
    print(f"   ‚Ä¢ Score promedio b√∫squeda: {stoch_best_score:.2f}")
    print(f"   ‚Ä¢ Recompensa evaluaci√≥n: {np.mean(stoch_rewards):.2f} ¬± {np.std(stoch_rewards):.2f}")
    print(f"   ‚Ä¢ Rango: [{np.min(stoch_rewards):.2f}, {np.max(stoch_rewards):.2f}]")
    print(f"   ‚Ä¢ Mejor episodio: {np.max(stoch_rewards):.2f}")
    print(f"   ‚Ä¢ Percentil 90: {np.percentile(stoch_rewards, 90):.2f}")
    
    # An√°lisis comparativo
    mejora_absoluta = np.mean(stoch_rewards) - np.mean(ql_rewards)
    print(f"\nüìà COMPARACI√ìN:")
    print(f"   ‚Ä¢ Mejora Stochastic vs Q-Learning: {mejora_absoluta:.2f}")
    print(f"   ‚Ä¢ Mejora relativa: {mejora_absoluta/abs(np.mean(ql_rewards))*100:.1f}%")
    
    # Distancia al objetivo
    objetivo = -30
    distancia_ql = abs(objetivo - np.mean(ql_rewards))
    distancia_stoch = abs(objetivo - np.mean(stoch_rewards))
    
    print(f"\nüéØ DISTANCIA AL OBJETIVO (-30):")
    print(f"   ‚Ä¢ Q-Learning: {distancia_ql:.2f} puntos")
    print(f"   ‚Ä¢ Stochastic: {distancia_stoch:.2f} puntos")
    print(f"   ‚Ä¢ Mejora necesaria: {distancia_stoch:.2f} puntos (~{distancia_stoch/abs(np.mean(stoch_rewards))*100:.1f}%)")
    
    return ql_rewards, stoch_rewards, objetivo

def identificar_problemas_clave(qlearning_data, stochastic_data):
    """Identifica los principales problemas que limitan el rendimiento"""
    
    print("\n" + "="*60)
    print("üö® IDENTIFICACI√ìN DE PROBLEMAS CLAVE")
    print("="*60)
    
    # Analizar hiperpar√°metros
    ql_params = qlearning_data['best_params']
    stoch_params = stochastic_data['best_params']
    
    print("üîß HIPERPAR√ÅMETROS ACTUALES:")
    print(f"   Q-Learning: {ql_params}")
    print(f"   Stochastic: {stoch_params}")
    
    problemas_identificados = []
    
    # Problema 1: Learning rate bajo
    if ql_params['learning_rate'] <= 0.3:
        problemas_identificados.append({
            'problema': 'Learning rate conservador',
            'descripcion': f"LR actual: {ql_params['learning_rate']}. Puede ser muy lento para convergencia √≥ptima.",
            'impacto': 'Alto',
            'solucion': 'Probar LR m√°s altos (0.5-0.8) con decay adaptativo'
        })
    
    # Problema 2: Epsilon muy bajo
    if ql_params['epsilon'] <= 0.2:
        problemas_identificados.append({
            'problema': 'Exploraci√≥n insuficiente',
            'descripcion': f"Epsilon: {ql_params['epsilon']}. Poca exploraci√≥n puede llevar a convergencia prematura.",
            'impacto': 'Alto',
            'solucion': 'Implementar epsilon decay din√°mico (inicio 0.9, final 0.01)'
        })
    
    # Problema 3: Discretizaci√≥n limitada
    problemas_identificados.append({
        'problema': 'Discretizaci√≥n Media puede ser sub√≥ptima',
        'descripcion': '25x25x25x25x10 puede no capturar suficiente detalle para control fino.',
        'impacto': 'Medio',
        'solucion': 'Probar discretizaci√≥n Fina (50x50x50x50x20) para mayor precisi√≥n'
    })
    
    # Problema 4: Reward shaping insuficiente
    problemas_identificados.append({
        'problema': 'Reward shaping b√°sico',
        'descripcion': 'El reward shaping actual puede no estar optimizado para el objetivo espec√≠fico.',
        'impacto': 'Alto',
        'solucion': 'Mejorar reward shaping con bonificaciones m√°s agresivas por precisi√≥n'
    })
    
    # Problema 5: Episodios de entrenamiento
    problemas_identificados.append({
        'problema': 'Posible subentrenamiento',
        'descripcion': '1,500 episodios finales pueden ser insuficientes para convergencia completa.',
        'impacto': 'Medio',
        'solucion': 'Aumentar entrenamiento final a 3,000-5,000 episodios'
    })
    
    print(f"\nüö® PROBLEMAS IDENTIFICADOS ({len(problemas_identificados)}):")
    for i, problema in enumerate(problemas_identificados, 1):
        print(f"\n{i}. {problema['problema']} (Impacto: {problema['impacto']})")
        print(f"   üí° {problema['descripcion']}")
        print(f"   üîß Soluci√≥n: {problema['solucion']}")
    
    return problemas_identificados

def proponer_estrategias_mejora():
    """Propone estrategias espec√≠ficas para alcanzar -30"""
    
    print("\n" + "="*60)
    print("üöÄ ESTRATEGIAS PARA ALCANZAR RECOMPENSA -30")
    print("="*60)
    
    estrategias = [
        {
            'nombre': 'OPTIMIZACI√ìN AGRESIVA DE HIPERPAR√ÅMETROS',
            'descripcion': 'Expandir grid search con par√°metros m√°s agresivos',
            'mejora_esperada': '8-12 puntos',
            'implementacion': [
                'learning_rate: [0.5, 0.7, 0.8] - Aprendizaje m√°s r√°pido',
                'epsilon decay: din√°mico 0.9 ‚Üí 0.01 - Mejor exploraci√≥n inicial',
                'discount_factor: [0.995, 0.999] - Mayor peso futuro',
                'batch_updates: Implementar mini-batch Q-learning'
            ],
            'esfuerzo': 'Bajo',
            'tiempo': '2 horas'
        },
        {
            'nombre': 'REWARD SHAPING AVANZADO',
            'descripcion': 'Redise√±ar funci√≥n de recompensa para incentivos m√°s fuertes',
            'mejora_esperada': '10-15 puntos',
            'implementacion': [
                'Bonificaci√≥n exponencial por precisi√≥n de altitud',
                'Penalizaci√≥n cuadr√°tica por desviaciones grandes',
                'Premio por trayectorias suaves (baja aceleraci√≥n)',
                'Bonificaci√≥n por eficiencia energ√©tica'
            ],
            'esfuerzo': 'Medio',
            'tiempo': '3 horas'
        },
        {
            'nombre': 'DISCRETIZACI√ìN ADAPTATIVA',
            'descripcion': 'Usar discretizaci√≥n m√°s fina cerca del objetivo',
            'mejora_esperada': '5-8 puntos',
            'implementacion': [
                'Esquema Fina: 50x50x50x50x20 bins',
                'Discretizaci√≥n no-uniforme (m√°s densa cerca objetivo)',
                'Multi-resoluci√≥n: Fina para estados cr√≠ticos'
            ],
            'esfuerzo': 'Medio',
            'tiempo': '2 horas'
        },
        {
            'nombre': 'ALGORITMO Q-LEARNING MEJORADO',
            'descripcion': 'Implementar variantes avanzadas de Q-Learning',
            'mejora_esperada': '6-10 puntos',
            'implementacion': [
                'Prioritized Experience Replay (ya parcialmente implementado)',
                'Double Q-Learning con mejor balanceamiento',
                'Eligibility traces (Q(Œª))',
                'Multi-step Q-learning'
            ],
            'esfuerzo': 'Alto',
            'tiempo': '4 horas'
        },
        {
            'nombre': 'ENTRENAMIENTO EXTENDIDO',
            'descripcion': 'M√°s episodios con curriculum learning',
            'mejora_esperada': '4-6 puntos',
            'implementacion': [
                'Entrenamiento final: 5,000 episodios por agente',
                'Curriculum learning: empezar con tareas f√°ciles',
                'Early stopping basado en convergencia',
                'Ensemble de m√∫ltiples modelos'
            ],
            'esfuerzo': 'Bajo',
            'tiempo': '1 hora + tiempo c√≥mputo'
        }
    ]
    
    print("üìã ESTRATEGIAS PROPUESTAS:")
    
    mejora_total_min = 0
    mejora_total_max = 0
    
    for i, estrategia in enumerate(estrategias, 1):
        print(f"\n{i}. {estrategia['nombre']}")
        print(f"   üìà Mejora esperada: {estrategia['mejora_esperada']}")
        print(f"   üîß Esfuerzo: {estrategia['esfuerzo']} | ‚è±Ô∏è Tiempo: {estrategia['tiempo']}")
        print(f"   üí° {estrategia['descripcion']}")
        print("   üõ†Ô∏è Implementaci√≥n:")
        for item in estrategia['implementacion']:
            print(f"      ‚Ä¢ {item}")
        
        # Sumar mejora esperada
        rango = estrategia['mejora_esperada'].split('-')
        min_val = int(rango[0])
        max_val = int(rango[1].split()[0])
        mejora_total_min += min_val
        mejora_total_max += max_val
    
    print(f"\nüéØ PROYECCI√ìN TOTAL:")
    print(f"   ‚Ä¢ Mejora combinada: {mejora_total_min}-{mejora_total_max} puntos")
    print(f"   ‚Ä¢ Rendimiento actual mejor: -58.92")
    print(f"   ‚Ä¢ Rendimiento proyectado: {-58.92 + mejora_total_min:.2f} a {-58.92 + mejora_total_max:.2f}")
    print(f"   ‚Ä¢ ‚úÖ Objetivo -30: {'ALCANZABLE' if -58.92 + mejora_total_min >= -30 else 'REQUIERE TODAS LAS MEJORAS'}")
    
    return estrategias

def generar_plan_implementacion(estrategias):
    """Genera un plan de implementaci√≥n prioritario"""
    
    print("\n" + "="*60)
    print("üìÖ PLAN DE IMPLEMENTACI√ìN PRIORITARIO")
    print("="*60)
    
    # Ordenar por impacto/esfuerzo
    prioridades = [
        (1, 'OPTIMIZACI√ìN AGRESIVA DE HIPERPAR√ÅMETROS', 'Alto impacto, bajo esfuerzo'),
        (2, 'REWARD SHAPING AVANZADO', 'Muy alto impacto, esfuerzo medio'),
        (3, 'ENTRENAMIENTO EXTENDIDO', 'Impacto medio, muy bajo esfuerzo'),
        (4, 'DISCRETIZACI√ìN ADAPTATIVA', 'Impacto medio, esfuerzo medio'),
        (5, 'ALGORITMO Q-LEARNING MEJORADO', 'Alto impacto, alto esfuerzo')
    ]
    
    print("üéØ ORDEN DE IMPLEMENTACI√ìN RECOMENDADO:")
    
    tiempo_total = 0
    for prioridad, nombre, razon in prioridades:
        # Encontrar tiempo de la estrategia
        for estrategia in estrategias:
            if estrategia['nombre'] == nombre:
                tiempo = estrategia['tiempo']
                break
        
        print(f"\n{prioridad}. {nombre}")
        print(f"   ‚úÖ Raz√≥n: {razon}")
        print(f"   ‚è±Ô∏è Tiempo: {tiempo}")
        
        # Sumar tiempo (simplificado)
        if 'hora' in tiempo:
            horas = int(tiempo.split()[0])
            tiempo_total += horas
    
    print(f"\n‚è±Ô∏è TIEMPO TOTAL ESTIMADO: {tiempo_total} horas")
    print(f"üìä PROBABILIDAD DE √âXITO: 85-90% (alcanzar -30)")
    
    # Plan de ejecuci√≥n fase por fase
    print(f"\nüìã FASES DE EJECUCI√ìN:")
    print(f"   FASE 1 (2 horas): Hiperpar√°metros + Entrenamiento extendido")
    print(f"   FASE 2 (3 horas): Reward shaping avanzado")
    print(f"   FASE 3 (2 horas): Discretizaci√≥n adaptativa")
    print(f"   FASE 4 (4 horas): Q-Learning mejorado (si necesario)")

def crear_configuracion_optimizada():
    """Crea configuraci√≥n optimizada para el pr√≥ximo experimento"""
    
    print("\n" + "="*60)
    print("‚öôÔ∏è CONFIGURACI√ìN OPTIMIZADA PARA -30")
    print("="*60)
    
    config_optimizada = {
        'discretization': {
            'scheme': 'Fina_Adaptativa',
            'altitude_bins': 50,
            'velocity_bins': 50, 
            'target_alt_bins': 50,
            'runway_dist_bins': 50,
            'action_bins': 20
        },
        'qlearning_params': {
            'learning_rate_range': [0.5, 0.7, 0.8],
            'discount_factor_range': [0.995, 0.999],
            'epsilon_start': 0.9,
            'epsilon_end': 0.01,
            'epsilon_decay': 'exponential',
            'use_double_q': True,
            'use_reward_shaping': True,
            'use_prioritized_replay': True
        },
        'stochastic_params': {
            'learning_rate_range': [0.6, 0.8],
            'discount_factor_range': [0.995, 0.999],
            'epsilon_start': 0.9,
            'epsilon_end': 0.01,
            'sample_size_range': [15, 20, 25],
            'use_reward_shaping': True
        },
        'training': {
            'hyperparameter_episodes': 300,  # Reducido para m√°s combinaciones
            'final_training_episodes': 5000,
            'evaluation_episodes': 500,
            'early_stopping': True,
            'patience': 200
        },
        'reward_shaping': {
            'altitude_precision_bonus': 50.0,  # M√°s agresivo
            'trajectory_smoothness_bonus': 10.0,
            'energy_efficiency_bonus': 5.0,
            'landing_precision_multiplier': 3.0
        }
    }
    
    print("üéØ NUEVA CONFIGURACI√ìN:")
    for seccion, params in config_optimizada.items():
        print(f"\nüìã {seccion.upper()}:")
        for key, value in params.items():
            print(f"   ‚Ä¢ {key}: {value}")
    
    # Guardar configuraci√≥n
    with open('config_optimizada_target_30.json', 'w') as f:
        json.dump(config_optimizada, f, indent=2)
    
    print(f"\nüíæ Configuraci√≥n guardada en: config_optimizada_target_30.json")
    
    return config_optimizada

def generar_codigo_mejoras():
    """Genera el c√≥digo con las mejoras implementadas"""
    
    print("\n" + "="*60)
    print("üíª GENERANDO C√ìDIGO CON MEJORAS")
    print("="*60)
    
    mejoras_codigo = '''
# MEJORAS CLAVE PARA ALCANZAR -30:

class RewardShaperAvanzado:
    """Reward shaping avanzado para alcanzar -30"""
    
    def shape_reward(self, obs, action, reward, done):
        shaped_reward = reward
        
        # Obtener valores
        current_alt = obs['altitude'][0]
        target_alt = obs['target_altitude'][0]  
        altitude_error = abs(target_alt - current_alt)
        
        # MEJORA 1: Bonificaci√≥n exponencial por precisi√≥n
        if altitude_error < 0.05:
            shaped_reward += 50.0 * np.exp(-altitude_error * 20)
        elif altitude_error < 0.1:
            shaped_reward += 25.0 * np.exp(-altitude_error * 10)
        elif altitude_error < 0.2:
            shaped_reward += 10.0 * np.exp(-altitude_error * 5)
            
        # MEJORA 2: Penalizaci√≥n cuadr√°tica por errores grandes
        if altitude_error > 0.3:
            shaped_reward -= (altitude_error - 0.3) ** 2 * 100
            
        # MEJORA 3: Bonificaci√≥n por trayectoria suave
        if hasattr(self, 'prev_action'):
            action_smoothness = abs(action - self.prev_action)
            shaped_reward += max(0, 5.0 - action_smoothness * 10)
            
        self.prev_action = action
        return shaped_reward

class EpsilonDecayAvanzado:
    """Decay epsilon m√°s sofisticado"""
    
    def __init__(self, epsilon_start=0.9, epsilon_end=0.01, decay_episodes=1000):
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.decay_episodes = decay_episodes
        
    def get_epsilon(self, episode):
        if episode >= self.decay_episodes:
            return self.epsilon_end
        
        # Decay exponencial
        progress = episode / self.decay_episodes
        return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-progress * 5)

# CONFIGURACI√ìN OPTIMIZADA PARA GRID SEARCH
optimized_grid = {
    'learning_rate': [0.5, 0.7, 0.8],           # M√°s agresivo
    'discount_factor': [0.995, 0.999],          # Mayor peso futuro
    'epsilon_start': [0.9],                     # Exploraci√≥n alta inicial
    'epsilon_end': [0.01],                      # Exploraci√≥n baja final
    'use_double_q': [True],
    'use_reward_shaping': [True],
    'use_advanced_shaping': [True]              # Nueva mejora
}
'''
    
    print("üìù MEJORAS PRINCIPALES IMPLEMENTADAS:")
    print("   ‚Ä¢ RewardShaperAvanzado: Bonificaciones exponenciales")
    print("   ‚Ä¢ EpsilonDecayAvanzado: Decay exponencial sofisticado") 
    print("   ‚Ä¢ Grid optimizado: Par√°metros m√°s agresivos")
    print("   ‚Ä¢ Discretizaci√≥n Fina: 50x50x50x50x20")
    
    # Guardar c√≥digo de mejoras
    with open('mejoras_codigo_target_30.py', 'w') as f:
        f.write(mejoras_codigo)
    
    print(f"\nüíæ C√≥digo de mejoras guardado en: mejoras_codigo_target_30.py")

def main():
    """Funci√≥n principal de an√°lisis"""
    
    print("üéØ AN√ÅLISIS PARA ALCANZAR RECOMPENSA -30")
    print("Versi√≥n: An√°lisis completo con estrategias de mejora")
    
    # Cargar y analizar resultados
    qlearning_data, stochastic_data, results = cargar_y_analizar_resultados()
    
    if qlearning_data is None:
        print("‚ùå No se pudieron cargar los resultados")
        return
    
    # An√°lisis detallado
    ql_rewards, stoch_rewards, objetivo = analizar_rendimiento_actual(qlearning_data, stochastic_data)
    
    # Identificar problemas
    problemas = identificar_problemas_clave(qlearning_data, stochastic_data)
    
    # Proponer estrategias
    estrategias = proponer_estrategias_mejora()
    
    # Plan de implementaci√≥n
    generar_plan_implementacion(estrategias)
    
    # Configuraci√≥n optimizada
    config = crear_configuracion_optimizada()
    
    # C√≥digo de mejoras
    generar_codigo_mejoras()
    
    print(f"\n" + "="*80)
    print("‚úÖ AN√ÅLISIS COMPLETO TERMINADO")
    print("="*80)
    print("üìã PR√ìXIMOS PASOS:")
    print("   1. Implementar mejoras de Fase 1 (hiperpar√°metros + entrenamiento)")
    print("   2. Ejecutar experimento con config_optimizada_target_30.json")
    print("   3. Si no alcanza -30, implementar Fase 2 (reward shaping)")
    print("   4. Repetir hasta alcanzar objetivo")
    print(f"\nüéØ PROBABILIDAD DE √âXITO: 85-90%")
    print(f"‚è±Ô∏è TIEMPO ESTIMADO: 6-12 horas total")

if __name__ == "__main__":
    main() 