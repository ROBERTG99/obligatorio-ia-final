#!/usr/bin/env python3
"""
üöÄ EXPERIMENTO EXPRESS FLAN - VERSI√ìN OPTIMIZADA PARA TIEMPO
Experimento completo pero con episodios reducidos para validaci√≥n r√°pida
"""

import os
import time
import sys
import json
import psutil
from datetime import datetime, timedelta
from contextlib import redirect_stdout, redirect_stderr

def estimate_express_time():
    """Estima tiempo del experimento express"""
    print("‚è±Ô∏è EXPERIMENTO EXPRESS - ESTIMACI√ìN DE TIEMPO")
    print("=" * 60)
    
    # Configuraci√≥n express
    hyperparams_episodes = 1000    # Era 6,000
    final_training = 5000         # Era 60,000  
    final_evaluation = 500        # Era 8,000
    
    total_episodes = (
        4 * hyperparams_episodes +  # Q-Learning hyperparams
        8 * hyperparams_episodes +  # Stochastic hyperparams
        2 * final_training +        # Entrenamiento final
        2 * final_evaluation        # Evaluaci√≥n final
    )
    
    # Basado en datos reales: 1.699 segundos por episodio
    time_per_episode = 1.7  # segundos
    total_time_hours = (total_episodes * time_per_episode) / 3600
    
    print(f"üìä CONFIGURACI√ìN EXPRESS:")
    print(f"   ‚Ä¢ Hiperpar√°metros: {hyperparams_episodes:,} episodios por combinaci√≥n")
    print(f"   ‚Ä¢ Entrenamiento final: {final_training:,} episodios por agente")
    print(f"   ‚Ä¢ Evaluaci√≥n: {final_evaluation:,} episodios por agente")
    print(f"   ‚Ä¢ TOTAL: {total_episodes:,} episodios")
    print()
    print(f"‚è∞ TIEMPO ESTIMADO:")
    print(f"   ‚Ä¢ Tiempo por episodio: {time_per_episode} segundos")
    print(f"   ‚Ä¢ Tiempo total: {total_time_hours:.1f} horas")
    
    start_time = datetime.now()
    estimated_end = start_time + timedelta(hours=total_time_hours)
    
    print(f"   ‚Ä¢ Inicio: {start_time.strftime('%H:%M:%S')}")
    print(f"   ‚Ä¢ Finalizaci√≥n: {estimated_end.strftime('%H:%M:%S')}")
    print(f"   ‚Ä¢ Duraci√≥n: {estimated_end - start_time}")
    print()
    
    return total_time_hours

def run_express_experiment():
    """Ejecuta el experimento express"""
    print("üöÄ INICIANDO EXPERIMENTO EXPRESS")
    print("=" * 80)
    
    # Verificar DescentEnv REAL
    try:
        from descent_env import DescentEnv
        print("‚úÖ DescentEnv REAL confirmado")
    except ImportError:
        print("‚ùå ERROR: DescentEnv no disponible")
        return False
    
    print("\nüèÉ Ejecutando experimento express...")
    start_time = time.time()
    
    try:
        # Importar componentes principales
        from flan_qlearning_solution import (
            DescentEnv, DiscretizationScheme, QLearningAgent, 
            StochasticQLearningAgent, QLearningTrainer,
            PerformanceEvaluator, HyperparameterOptimizer,
            CONFIG_AUTOMATICO
        )
        
        print("üéØ Creando entorno y configuraci√≥n...")
        
        # Crear entorno (suprimir warnings)
        with open(os.devnull, 'w') as devnull:
            with redirect_stdout(devnull), redirect_stderr(devnull):
                env = DescentEnv(render_mode=None)
        
        # Esquema de discretizaci√≥n optimizado (pero m√°s peque√±o)
        discretization = DiscretizationScheme("Express", 20, 15, 15, 15, 10)
        
        # CONFIGURACI√ìN EXPRESS
        EXPRESS_CONFIG = {
            'hyperparams_episodes': 1000,    # vs 6,000 original
            'final_training': 5000,          # vs 60,000 original  
            'final_evaluation': 500,         # vs 8,000 original
        }
        
        print(f"üìä Configuraci√≥n Express: {EXPRESS_CONFIG}")
        
        results_summary = {}
        
        # === OPTIMIZACI√ìN Q-LEARNING ===
        print("\n1. üîç Optimizando Q-Learning (Express)...")
        
        # Grid search con menos combinaciones y episodios
        param_grid_ql = {
            'learning_rate': [0.8, 0.95],      # vs [0.5, 0.7] original
            'discount_factor': [0.999],
            'epsilon': [0.8, 0.95],            # vs [0.7, 0.8] original
            'use_double_q': [True],
            'use_reward_shaping': [True]
        }
        
        # Crear optimizador express
        optimizer = HyperparameterOptimizer(env, discretization)
        
        # Modificar temporalmente el train_and_evaluate_agent para express
        original_training = 6000
        optimizer.express_episodes = EXPRESS_CONFIG['hyperparams_episodes']
        
        # Ejecutar grid search
        qlearning_results = optimizer.grid_search('qlearning', param_grid_ql)
        print(f"‚úÖ Mejores par√°metros Q-Learning: {qlearning_results['best_params']}")
        
        # === ENTRENAMIENTO FINAL Q-LEARNING ===
        print(f"\n2. üèãÔ∏è Entrenando Q-Learning final ({EXPRESS_CONFIG['final_training']} episodios)...")
        
        best_qlearning_agent = QLearningAgent(discretization, **qlearning_results['best_params'])
        qlearning_trainer = QLearningTrainer(env, best_qlearning_agent, discretization)
        qlearning_trainer.train(episodes=EXPRESS_CONFIG['final_training'], verbose=True)
        
        # === EVALUACI√ìN Q-LEARNING ===
        print(f"\n3. üìä Evaluando Q-Learning ({EXPRESS_CONFIG['final_evaluation']} episodios)...")
        
        qlearning_evaluator = PerformanceEvaluator(env, best_qlearning_agent, discretization)
        qlearning_eval = qlearning_evaluator.evaluate_multiple_episodes(
            num_episodes=EXPRESS_CONFIG['final_evaluation']
        )
        
        # === OPTIMIZACI√ìN STOCHASTIC ===
        print("\n4. üîç Optimizando Stochastic Q-Learning (Express)...")
        
        param_grid_stoch = {
            'learning_rate': [0.6, 0.8],
            'discount_factor': [0.999],
            'epsilon': [0.8, 0.9],
            'sample_size': [10, 12],
            'use_reward_shaping': [True]
        }
        
        stochastic_results = optimizer.grid_search('stochastic', param_grid_stoch)
        print(f"‚úÖ Mejores par√°metros Stochastic: {stochastic_results['best_params']}")
        
        # === ENTRENAMIENTO FINAL STOCHASTIC ===
        print(f"\n5. üèãÔ∏è Entrenando Stochastic final ({EXPRESS_CONFIG['final_training']} episodios)...")
        
        best_stochastic_agent = StochasticQLearningAgent(discretization, **stochastic_results['best_params'])
        stochastic_trainer = QLearningTrainer(env, best_stochastic_agent, discretization)
        stochastic_trainer.train(episodes=EXPRESS_CONFIG['final_training'], verbose=True)
        
        # === EVALUACI√ìN STOCHASTIC ===
        print(f"\n6. üìä Evaluando Stochastic ({EXPRESS_CONFIG['final_evaluation']} episodios)...")
        
        stochastic_evaluator = PerformanceEvaluator(env, best_stochastic_agent, discretization)
        stochastic_eval = stochastic_evaluator.evaluate_multiple_episodes(
            num_episodes=EXPRESS_CONFIG['final_evaluation']
        )
        
        # === GUARDAR RESULTADOS ===
        print("\n7. üíæ Guardando resultados...")
        
        # Crear directorio de modelos
        models_dir = "models_express_flan"
        os.makedirs(models_dir, exist_ok=True)
        
        # Guardar modelos
        import pickle
        with open(f"{models_dir}/qlearning_agent.pkl", 'wb') as f:
            pickle.dump(best_qlearning_agent, f)
        
        with open(f"{models_dir}/stochastic_agent.pkl", 'wb') as f:
            pickle.dump(best_stochastic_agent, f)
        
        # Preparar resultados para JSON
        results_summary = {
            'Express': {
                'qlearning': {
                    'best_params': qlearning_results['best_params'],
                    'best_score': qlearning_results['best_score'],
                    'evaluation': {k: [float(x) for x in v] for k, v in qlearning_eval.items()},
                    'training_rewards': [float(x) for x in qlearning_trainer.training_rewards]
                },
                'stochastic': {
                    'best_params': stochastic_results['best_params'],
                    'best_score': stochastic_results['best_score'],
                    'evaluation': {k: [float(x) for x in v] for k, v in stochastic_eval.items()},
                    'training_rewards': [float(x) for x in stochastic_trainer.training_rewards]
                }
            }
        }
        
        # Agregar metadatos
        results_summary['experiment_info'] = {
            'type': 'EXPRESS_EXPERIMENT',
            'objective': 'Validaci√≥n r√°pida de mejoras autom√°ticas',
            'config': EXPRESS_CONFIG,
            'environment': 'DescentEnv_REAL',
            'mejoras_aplicadas': [
                'RewardShaperAutomatico - supervivencia cr√≠tica',
                'Hiperpar√°metros ultra agresivos autom√°ticos',
                'Discretizaci√≥n optimizada express',
                'Grid search focalizados en mejores par√°metros',
                'Entrenamiento con mejoras autom√°ticas',
            ]
        }
        
        # Guardar JSON
        with open('flan_results_express.json', 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # === REPORTE FINAL ===
        print(f"\n‚úÖ EXPERIMENTO EXPRESS COMPLETADO")
        print("=" * 80)
        print(f"‚è±Ô∏è Tiempo total: {total_time / 3600:.1f} horas")
        print(f"üìä Resultados guardados en: flan_results_express.json")
        print(f"üéØ Modelos guardados en: {models_dir}/")
        
        # Mostrar resultados clave
        ql_reward = qlearning_eval['total_rewards']
        stoch_reward = stochastic_eval['total_rewards']
        
        print(f"\nüìà RESULTADOS CLAVE:")
        print(f"   Q-Learning promedio: {sum(ql_reward)/len(ql_reward):.2f}")
        print(f"   Stochastic promedio: {sum(stoch_reward)/len(stoch_reward):.2f}")
        print(f"   Supervivencia Q-Learning: {sum(qlearning_eval['survival_times'])/len(qlearning_eval['survival_times']):.1f} pasos")
        print(f"   Supervivencia Stochastic: {sum(stochastic_eval['survival_times'])/len(stochastic_eval['survival_times']):.1f} pasos")
        
        env.close()
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR en experimento express: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Funci√≥n principal del experimento express"""
    print("üéØ FLAN - EXPERIMENTO EXPRESS")
    print("=" * 80)
    print("üî• DescentEnv REAL + Mejoras Autom√°ticas (Versi√≥n Optimizada)")
    print("üìà OBJETIVO: Validar mejoras autom√°ticas r√°pidamente")
    print()
    
    # Informaci√≥n del sistema
    print("üñ•Ô∏è INFORMACI√ìN DEL SISTEMA")
    print("-" * 40)
    print(f"CPU cores: {psutil.cpu_count(logical=True)} l√≥gicos")
    print(f"RAM disponible: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    print()
    
    # Estimaci√≥n de tiempo
    estimated_hours = estimate_express_time()
    
    # Confirmaci√≥n
    print("üö® CONFIGURACI√ìN EXPRESS")
    print("=" * 50)
    print("‚Ä¢ Entorno: DescentEnv REAL (BlueSky)")
    print("‚Ä¢ Mejoras: Autom√°ticas integradas")
    print("‚Ä¢ Episodios: ~23,000 (vs 256,000 completo)")
    print(f"‚Ä¢ Tiempo: ~{estimated_hours:.1f} horas (vs 120+ completo)")
    print("‚Ä¢ Prop√≥sito: Validaci√≥n cient√≠fica r√°pida")
    print()
    
    confirm = input("¬øProceder con experimento EXPRESS? (y/N): ")
    if confirm.lower() != 'y':
        print("‚ùå Experimento cancelado")
        return
    
    # Ejecutar
    success = run_express_experiment()
    
    if success:
        print("\nüéâ EXPERIMENTO EXPRESS COMPLETADO")
        print("üìä Revisa flan_results_express.json para resultados")
        print("\nüöÄ SIGUIENTES PASOS:")
        print("   1. Analizar resultados express")
        print("   2. Si funciona bien, ejecutar experimento completo")
        print("   3. O ajustar configuraci√≥n bas√°ndose en resultados")
    else:
        print("\n‚ùå El experimento express fall√≥")

if __name__ == "__main__":
    main() 