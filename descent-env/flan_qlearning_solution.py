import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from contextlib import redirect_stderr, redirect_stdout

# OPTIMIZACI√ìN: Usar DescentEnv real para m√°ximo rendimiento
print("üöÄ CONFIGURACI√ìN OPTIMIZADA: Priorizando DescentEnv real para m√°ximo rendimiento")

# Intentar importar DescentEnv real primero (verificado funcionando)
try:
    from descent_env import DescentEnv
    BLUESKY_AVAILABLE = True
    print("‚úÖ DescentEnv real cargado exitosamente - M√ÅXIMO RENDIMIENTO")
except ImportError as e:
    print(f"‚ö†Ô∏è  DescentEnv no disponible: {e}")
    DescentEnv = None
    BLUESKY_AVAILABLE = False

# MockDescentEnv solo como fallback extremo
try:
    from mock_descent_env import MockDescentEnv
    MOCK_AVAILABLE = True
except ImportError:
    print("Warning: MockDescentEnv tampoco disponible")
    MockDescentEnv = None
    MOCK_AVAILABLE = False

# Configuraci√≥n optimizada: DescentEnv real siempre que sea posible
if BLUESKY_AVAILABLE and DescentEnv is not None:
    print("üéØ USANDO DESCENTENV REAL - Rendimiento √≥ptimo garantizado")
    # DescentEnv ya importado
elif MOCK_AVAILABLE and MockDescentEnv is not None:
    print("üìÑ Fallback: Usando MockDescentEnv")
    DescentEnv = MockDescentEnv
else:
    raise ImportError("‚ùå Error cr√≠tico: Ning√∫n entorno disponible")

import random
from collections import deque, namedtuple
import time
from typing import Dict, List, Tuple, Any, Optional, Union
import json
import pickle
import os
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import psutil
import signal
import sys
import heapq

# Variable global para manejo de se√±ales
STOP_EXECUTION = False

def signal_handler(signum, frame):
    """Manejador de se√±ales para parada graceful"""
    global STOP_EXECUTION
    print("\n\nRecibida se√±al de interrupci√≥n. Deteniendo ejecuci√≥n...")
    STOP_EXECUTION = True
    sys.exit(0)

# Registrar manejador de se√±ales
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# Estructura para Prioritized Experience Replay
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done', 'priority'])

class DiscretizationScheme:
    """Clase para manejar diferentes esquemas de discretizaci√≥n"""
    
    def __init__(self, name: str, 
                 altitude_bins: int, 
                 velocity_bins: int, 
                 target_alt_bins: int, 
                 runway_dist_bins: int,
                 action_bins: int):
        self.name = name
        self.altitude_bins = altitude_bins
        self.velocity_bins = velocity_bins
        self.target_alt_bins = target_alt_bins
        self.runway_dist_bins = runway_dist_bins
        self.action_bins = action_bins
        
        # Definir rangos de discretizaci√≥n basados en el an√°lisis del entorno
        self.altitude_space = np.linspace(-2, 2, altitude_bins)  # Normalizado
        self.velocity_space = np.linspace(-3, 3, velocity_bins)  # Normalizado
        self.target_alt_space = np.linspace(-2, 2, target_alt_bins)  # Normalizado
        self.runway_dist_space = np.linspace(-2, 2, runway_dist_bins)  # Normalizado
        self.action_space = np.linspace(-1, 1, action_bins)
        
    def get_state(self, obs: Dict) -> Tuple[int, int, int, int]:
        """Convierte observaci√≥n continua en estado discreto"""
        alt = obs['altitude'][0]
        vz = obs['vz'][0]
        target_alt = obs['target_altitude'][0]
        runway_dist = obs['runway_distance'][0]
        
        # Clamp valores fuera de rango
        alt = np.clip(alt, -2, 2)
        vz = np.clip(vz, -3, 3)
        target_alt = np.clip(target_alt, -2, 2)
        runway_dist = np.clip(runway_dist, -2, 2)
        
        alt_idx = np.digitize(alt, self.altitude_space) - 1
        vz_idx = np.digitize(vz, self.velocity_space) - 1
        target_alt_idx = np.digitize(target_alt, self.target_alt_space) - 1
        runway_dist_idx = np.digitize(runway_dist, self.runway_dist_space) - 1
        
        # Asegurar √≠ndices v√°lidos
        alt_idx = np.clip(alt_idx, 0, self.altitude_bins - 1)
        vz_idx = np.clip(vz_idx, 0, self.velocity_bins - 1)
        target_alt_idx = np.clip(target_alt_idx, 0, self.target_alt_bins - 1)
        runway_dist_idx = np.clip(runway_dist_idx, 0, self.runway_dist_bins - 1)
        
        return alt_idx, vz_idx, target_alt_idx, runway_dist_idx
    
    def get_action_index(self, action: float) -> int:
        """Convierte acci√≥n continua en √≠ndice discreto"""
        action = np.clip(action, -1, 1)
        action_idx = np.digitize(action, self.action_space) - 1
        return int(np.clip(action_idx, 0, self.action_bins - 1))
    
    def get_action_from_index(self, action_idx: int) -> float:
        """Convierte √≠ndice discreto en acci√≥n continua"""
        return self.action_space[action_idx]

class QLearningAgent:
    """Agente Q-Learning mejorado con reward shaping y t√©cnicas avanzadas"""
    
    def __init__(self, discretization: DiscretizationScheme, 
                 learning_rate: float = 0.1, 
                 discount_factor: float = 0.99, 
                 epsilon: float = 0.1,
                 use_double_q: bool = True,
                 use_reward_shaping: bool = True):
        self.discretization = discretization
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.use_double_q = use_double_q
        self.use_reward_shaping = use_reward_shaping
        
        # Inicializar tablas Q (doble para Double Q-Learning)
        shape = (discretization.altitude_bins, 
                discretization.velocity_bins, 
                discretization.target_alt_bins, 
                discretization.runway_dist_bins, 
                discretization.action_bins)
        self.Q1 = np.zeros(shape)
        self.Q2 = np.zeros(shape) if use_double_q else None
        
        # Contador de visitas para learning rate adaptativo
        self.visits = np.zeros(shape)
        
        # Reward shaper - MEJORA: Usar reward shaper agresivo para target -30
        self.reward_shaper = RewardShaperTarget30() if use_reward_shaping else None
        
        # Epsilon decay
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        
    def get_action(self, state: Tuple[int, int, int, int], training: bool = True) -> float:
        """Selecciona acci√≥n usando pol√≠tica epsilon-greedy mejorada"""
        if training and np.random.random() < self.epsilon:
            # Exploraci√≥n: acci√≥n aleatoria
            action_idx = np.random.randint(0, self.discretization.action_bins)
        else:
            # Explotaci√≥n: mejor acci√≥n seg√∫n Q promedio (si Double Q-Learning)
            if self.use_double_q and self.Q2 is not None:
                q_values = (self.Q1[state] + self.Q2[state]) / 2
            else:
                q_values = self.Q1[state]
            action_idx = int(np.argmax(q_values))
        
        return self.discretization.get_action_from_index(action_idx)
    
    def update(self, state: Tuple[int, int, int, int], 
               action: float, reward: float, 
               next_state: Tuple[int, int, int, int], 
               done: bool, obs: Optional[Dict] = None):
        """Actualiza la tabla Q usando t√©cnicas mejoradas"""
        action_idx = self.discretization.get_action_index(action)
        
        # Aplicar reward shaping si est√° habilitado
        if self.reward_shaper is not None and obs is not None:
            reward = self.reward_shaper.shape_reward(obs, action, reward, done)
        
        # Learning rate adaptativo
        self.visits[state][action_idx] += 1
        alpha = self.learning_rate / (1 + 0.05 * self.visits[state][action_idx])
        
        # Double Q-Learning o Q-Learning est√°ndar
        if self.use_double_q and self.Q2 is not None:
            # Alternar entre Q1 y Q2
            if np.random.random() < 0.5:
                # Actualizar Q1
                current_q = self.Q1[state][action_idx]
                if done:
                    target_q = reward
                else:
                    # Usar Q1 para seleccionar acci√≥n, Q2 para evaluar
                    best_action_idx = int(np.argmax(self.Q1[next_state]))
                    max_next_q = self.Q2[next_state][best_action_idx]
                    target_q = reward + self.discount_factor * max_next_q
                self.Q1[state][action_idx] = current_q + alpha * (target_q - current_q)
            else:
                # Actualizar Q2
                current_q = self.Q2[state][action_idx]
                if done:
                    target_q = reward
                else:
                    # Usar Q2 para seleccionar acci√≥n, Q1 para evaluar
                    best_action_idx = int(np.argmax(self.Q2[next_state]))
                    max_next_q = self.Q1[next_state][best_action_idx]
                    target_q = reward + self.discount_factor * max_next_q
                self.Q2[state][action_idx] = current_q + alpha * (target_q - current_q)
        else:
            # Q-Learning est√°ndar mejorado
            current_q = self.Q1[state][action_idx]
            if done:
                target_q = reward
            else:
                max_next_q = np.max(self.Q1[next_state])
                target_q = reward + self.discount_factor * max_next_q
            self.Q1[state][action_idx] = current_q + alpha * (target_q - current_q)
        
        # Epsilon decay
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def reset_episode(self):
        """Reset para nuevo episodio"""
        if self.reward_shaper is not None:
            self.reward_shaper.reset()

class StochasticQLearningAgent:
    """Agente Stochastic Q-Learning mejorado para espacios de acci√≥n grandes"""
    
    def __init__(self, discretization: DiscretizationScheme,
                 learning_rate: float = 0.1,
                 discount_factor: float = 0.99,
                 epsilon: float = 0.1,
                 sample_size: int = 10,
                 use_reward_shaping: bool = True):
        self.discretization = discretization
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.sample_size = sample_size
        self.use_reward_shaping = use_reward_shaping
        
        # Inicializar tabla Q
        shape = (discretization.altitude_bins, 
                discretization.velocity_bins, 
                discretization.target_alt_bins, 
                discretization.runway_dist_bins, 
                discretization.action_bins)
        self.Q = np.zeros(shape)
        
        # Contador de visitas
        self.visits = np.zeros(shape)
        
        # Reward shaper - MEJORA: Usar reward shaper agresivo para target -30
        self.reward_shaper = RewardShaperTarget30() if use_reward_shaping else None
        
        # Epsilon decay
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        
    def get_action(self, state: Tuple[int, int, int, int], training: bool = True) -> float:
        """Selecciona acci√≥n usando Stochastic Q-Learning mejorado"""
        # Asegurar que sample_size no sea mayor que action_bins
        effective_sample_size = min(self.sample_size, self.discretization.action_bins)
        
        if training and np.random.random() < self.epsilon:
            # Exploraci√≥n: muestrear acciones aleatorias
            sampled_indices = np.random.choice(
                self.discretization.action_bins, 
                size=effective_sample_size, 
                replace=False
            )
            # Seleccionar la mejor entre las muestreadas
            q_values = self.Q[state][sampled_indices]
            best_sampled_idx = sampled_indices[np.argmax(q_values)]
            action_idx = best_sampled_idx
        else:
            # Explotaci√≥n: muestrear y seleccionar la mejor
            sampled_indices = np.random.choice(
                self.discretization.action_bins, 
                size=effective_sample_size, 
                replace=False
            )
            q_values = self.Q[state][sampled_indices]
            best_sampled_idx = sampled_indices[np.argmax(q_values)]
            action_idx = best_sampled_idx
        
        return self.discretization.get_action_from_index(action_idx)
    
    def update(self, state: Tuple[int, int, int, int], 
               action: float, reward: float, 
               next_state: Tuple[int, int, int, int], 
               done: bool, obs: Optional[Dict] = None):
        """Actualiza la tabla Q usando Stochastic Q-Learning mejorado"""
        action_idx = self.discretization.get_action_index(action)
        
        # Aplicar reward shaping si est√° habilitado
        if self.reward_shaper is not None and obs is not None:
            reward = self.reward_shaper.shape_reward(obs, action, reward, done)
        
        # Learning rate adaptativo
        self.visits[state][action_idx] += 1
        alpha = self.learning_rate / (1 + 0.05 * self.visits[state][action_idx])
        
        # Stochastic Q-Learning update
        current_q = self.Q[state][action_idx]
        if done:
            target_q = reward
        else:
            # Muestrear acciones para el siguiente estado
            effective_sample_size = min(self.sample_size, self.discretization.action_bins)
            sampled_indices = np.random.choice(
                self.discretization.action_bins, 
                size=effective_sample_size, 
                replace=False
            )
            max_next_q = np.max(self.Q[next_state][sampled_indices])
            target_q = reward + self.discount_factor * max_next_q
        
        self.Q[state][action_idx] = current_q + alpha * (target_q - current_q)
        
        # Epsilon decay
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def reset_episode(self):
        """Reset para nuevo episodio"""
        if self.reward_shaper is not None:
            self.reward_shaper.reset()

class PerformanceEvaluator:
    """Evaluador de rendimiento del agente"""
    
    def __init__(self, env: Any, agent, discretization: DiscretizationScheme):
        self.env = env
        self.agent = agent
        self.discretization = discretization
        
    def evaluate_episode(self, render: bool = False) -> Dict[str, float]:
        """Eval√∫a un episodio completo"""
        obs, _ = self.env.reset()
        state = self.discretization.get_state(obs)
        
        total_reward = 0
        steps = 0
        max_altitude = -np.inf
        min_altitude = np.inf
        target_altitude = obs['target_altitude'][0]
        
        done = False
        while not done:
            action = self.agent.get_action(state, training=False)
            obs, reward, done, _, info = self.env.step(np.array([action]))
            next_state = self.discretization.get_state(obs)
            
            total_reward += reward
            steps += 1
            
            # Trackear m√©tricas
            current_alt = obs['altitude'][0]
            max_altitude = max(max_altitude, current_alt)
            min_altitude = min(min_altitude, current_alt)
            
            state = next_state
            
            if render:
                self.env.render()
        
        # Calcular m√©tricas adicionales
        altitude_error = abs(max_altitude - target_altitude)
        survival_time = steps
        
        return {
            'total_reward': total_reward,
            'steps': steps,
            'max_altitude': max_altitude,
            'min_altitude': min_altitude,
            'target_altitude': target_altitude,
            'altitude_error': altitude_error,
            'survival_time': survival_time,
            'final_altitude': info['final_altitude']
        }
    
    def evaluate_multiple_episodes(self, num_episodes: int = 100) -> Dict[str, List[float]]:
        """Eval√∫a m√∫ltiples episodios y retorna estad√≠sticas"""
        results = {
            'total_rewards': [],
            'steps': [],
            'altitude_errors': [],
            'survival_times': [],
            'final_altitudes': []
        }
        
        for _ in range(num_episodes):
            episode_result = self.evaluate_episode()
            results['total_rewards'].append(episode_result['total_reward'])
            results['steps'].append(episode_result['steps'])
            results['altitude_errors'].append(episode_result['altitude_error'])
            results['survival_times'].append(episode_result['survival_time'])
            results['final_altitudes'].append(episode_result['final_altitude'])
        
        return results

def train_and_evaluate_agent(params_tuple):
    """Funci√≥n para entrenar y evaluar un agente (para paralelizaci√≥n)"""
    agent_type, params, discretization_dict = params_tuple
    
    # Recrear discretizaci√≥n
    discretization = DiscretizationScheme(
        discretization_dict['name'],
        discretization_dict['altitude_bins'],
        discretization_dict['velocity_bins'],
        discretization_dict['target_alt_bins'],
        discretization_dict['runway_dist_bins'],
        discretization_dict['action_bins']
    )
    
    # CONFIGURACI√ìN OPTIMIZADA: DescentEnv real prioritario para m√°ximo rendimiento
    with open(os.devnull, 'w') as devnull:
        with redirect_stdout(devnull), redirect_stderr(devnull):
            if BLUESKY_AVAILABLE and DescentEnv is not None:
                env = DescentEnv(render_mode=None)
                print("üöÄ USANDO DESCENTENV REAL - M√°ximo rendimiento para entrenamiento final")
            elif MOCK_AVAILABLE and MockDescentEnv is not None:
                env = MockDescentEnv(render_mode=None)
                print("üìÑ Fallback: Usando MockDescentEnv para experimento")
            else:
                raise ImportError("‚ùå Error cr√≠tico: No hay entornos disponibles")
    
    # Crear agente
    if agent_type == 'qlearning':
        agent = QLearningAgent(discretization, **params)
    else:
        agent = StochasticQLearningAgent(discretization, **params)
    
    # OPTIMIZACI√ìN TARGET -30: Episodios extensivos para m√°ximo rendimiento
    TRAINING_EPISODES = 1500  # Entrenamiento intensivo para alcanzar -30
    trainer = QLearningTrainer(env, agent, discretization)
    trainer.train(episodes=TRAINING_EPISODES, verbose=False)
    
    # Evaluaci√≥n incluida en los 400 episodios
    EVALUATION_EPISODES = 50  # Evaluaci√≥n eficiente
    evaluator = PerformanceEvaluator(env, agent, discretization)
    eval_results = evaluator.evaluate_multiple_episodes(num_episodes=EVALUATION_EPISODES)
    
    # Calcular score
    score = np.mean(eval_results['total_rewards'])
    
    return {
        'params': params,
        'score': score,
        'eval_results': eval_results,
        'training_episodes': TRAINING_EPISODES,
        'evaluation_episodes': EVALUATION_EPISODES
    }

class HyperparameterOptimizer:
    """Optimizador de hiperpar√°metros con paralelizaci√≥n"""
    
    def __init__(self, env: Any, discretization: DiscretizationScheme):
        self.env = env
        self.discretization = discretization
        
    def grid_search(self, agent_type: str = 'qlearning', 
                   param_grid: Optional[Dict[str, List]] = None) -> Dict[str, Any]:
        """Realiza b√∫squeda en cuadr√≠cula de hiperpar√°metros con paralelizaci√≥n"""
        
        if param_grid is None:
            if agent_type == 'qlearning':
                # OPTIMIZACI√ìN TARGET -30: Grid AGRESIVO para m√°ximo rendimiento
                param_grid = {
                    'learning_rate': [0.7, 0.8, 0.9],        # Aprendizaje MUY r√°pido
                    'discount_factor': [0.999],               # M√°ximo peso futuro
                    'epsilon': [0.05],                        # Exploraci√≥n m√≠nima final
                    'use_double_q': [True],                   # Solo la mejor opci√≥n
                    'use_reward_shaping': [True]              # Siempre usar reward shaping
                }
            else:  # stochastic - OPTIMIZACI√ìN TARGET -30: Grid AGRESIVO
                param_grid = {
                    'learning_rate': [0.7, 0.8],           # Aprendizaje muy r√°pido
                    'discount_factor': [0.999],             # M√°ximo peso futuro
                    'epsilon': [0.05],                      # Exploraci√≥n m√≠nima
                    'sample_size': [15, 20],               # Muestreo m√°s amplio
                    'use_reward_shaping': [True]            # Siempre usar
                }
        
        # Calcular n√∫mero de combinaciones
        total_combinations = 1
        for param_values in param_grid.values():
            total_combinations *= len(param_values)
        
        print(f"Probando {total_combinations} combinaciones de hiperpar√°metros con paralelizaci√≥n...")
        
        # OPTIMIZACI√ìN 10K: Usar paralelizaci√≥n balanceada para evitar sobrecarga
        cpu_count = psutil.cpu_count(logical=True) or os.cpu_count()
        # Nuevas reglas: usar TODOS los cores l√≥gicos disponibles, con fallback a 2
        num_cores = cpu_count if cpu_count and cpu_count > 0 else 2
        print(f"‚ö° PARALELIZACI√ìN OPTIMIZADA: Usando TODOS los {num_cores} cores disponibles")
        
        # Generar todas las combinaciones
        import itertools
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        
        # Preparar datos para paralelizaci√≥n
        discretization_dict = {
            'name': self.discretization.name,
            'altitude_bins': self.discretization.altitude_bins,
            'velocity_bins': self.discretization.velocity_bins,
            'target_alt_bins': self.discretization.target_alt_bins,
            'runway_dist_bins': self.discretization.runway_dist_bins,
            'action_bins': self.discretization.action_bins
        }
        
        tasks = []
        for combination in itertools.product(*param_values):
            params = dict(zip(param_names, combination))
            tasks.append((agent_type, params, discretization_dict))
        
        # Ejecutar en paralelo
        results = []
        best_params = None
        best_score = -np.inf
        
        try:
            with ProcessPoolExecutor(max_workers=num_cores) as executor:
                # Enviar todas las tareas
                future_to_params = {executor.submit(train_and_evaluate_agent, task): task[1] 
                                   for task in tasks}
                
                # Procesar resultados conforme van complet√°ndose
                for i, future in enumerate(as_completed(future_to_params)):
                    if STOP_EXECUTION:
                        break
                        
                    try:
                        result = future.result()
                        results.append(result)
                        
                        if result['score'] > best_score:
                            best_score = result['score']
                            best_params = result['params']
                        
                        print(f"Completado {i+1}/{total_combinations}: {result['params']} -> Score: {result['score']:.2f}")
                        
                    except Exception as e:
                        params = future_to_params[future]
                        print(f"Error en combinaci√≥n {params}: {e}")
                        
        except KeyboardInterrupt:
            print("Interrumpido por el usuario")
            
        return {
            'best_params': best_params,
            'best_score': best_score,
            'all_results': results
        }

class QLearningTrainer:
    """Entrenador mejorado para agentes Q-Learning"""
    
    def __init__(self, env, agent, discretization: DiscretizationScheme):
        self.env = env
        self.agent = agent
        self.discretization = discretization
        self.training_rewards = []
        
    def train(self, episodes: int = 1000, verbose: bool = True) -> List[float]:
        """Entrena el agente con t√©cnicas mejoradas"""
        episode_rewards = []
        
        for episode in range(episodes):
            # Reset del agente para nuevo episodio
            if hasattr(self.agent, 'reset_episode'):
                self.agent.reset_episode()
                
            obs, _ = self.env.reset()
            state = self.discretization.get_state(obs)
            
            total_reward = 0
            done = False
            step = 0
            
            while not done and step < 500:  # L√≠mite de pasos
                action = self.agent.get_action(state, training=True)
                next_obs, reward, done, _, _ = self.env.step(np.array([action]))
                next_state = self.discretization.get_state(next_obs)
                
                # Actualizar agente con observaci√≥n para reward shaping
                if hasattr(self.agent, 'update'):
                    try:
                        # Intentar pasar observaci√≥n para reward shaping
                        self.agent.update(state, action, reward, next_state, done, next_obs)
                    except TypeError:
                        # Fallback para agentes que no soportan obs
                        self.agent.update(state, action, reward, next_state, done)
                
                total_reward += reward
                state = next_state
                step += 1
            
            episode_rewards.append(total_reward)
            
            if verbose and (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                current_epsilon = getattr(self.agent, 'epsilon', 'N/A')
                print(f"Episodio {episode + 1}, Recompensa promedio (√∫ltimos 100): {avg_reward:.2f}, "
                      f"Epsilon: {current_epsilon:.3f}")
        
        self.training_rewards = episode_rewards
        return episode_rewards

class PrioritizedReplayBuffer:
    """Buffer de experiencia con priorizaci√≥n para mejorar el aprendizaje"""
    
    def __init__(self, capacity: int = 10000, alpha: float = 0.6):
        self.capacity = capacity
        self.alpha = alpha  # Controla cu√°nto se usa la priorizaci√≥n
        self.buffer = []
        self.priorities = []
        self.pos = 0
        
    def add(self, state: Tuple, action: float, reward: float, 
            next_state: Tuple, done: bool, priority: Optional[float] = None):
        """Agregar experiencia al buffer"""
        if priority is None:
            priority = max(self.priorities) if self.priorities else 1.0
            
        experience = Experience(state, action, reward, next_state, done, priority)
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
            self.priorities.append(priority)
        else:
            self.buffer[self.pos] = experience
            self.priorities[self.pos] = priority
            
        self.pos = (self.pos + 1) % self.capacity
    
    def sample(self, batch_size: int, beta: float = 0.4):
        """Muestrear experiencias basado en prioridades"""
        if len(self.buffer) < batch_size:
            return []
            
        # Calcular probabilidades basadas en prioridades
        priorities = np.array(self.priorities[:len(self.buffer)])
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # Muestrear √≠ndices
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # Calcular importance sampling weights
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights /= weights.max()
        
        experiences = [self.buffer[i] for i in indices]
        return experiences, indices, weights
    
    def update_priorities(self, indices: List[int], priorities: List[float]):
        """Actualizar prioridades de experiencias"""
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority
    
    def __len__(self):
        return len(self.buffer)

class RewardShaper:
    """Clase para mejorar la funci√≥n de recompensa con reward shaping"""
    
    def __init__(self):
        self.prev_altitude_error = None
        self.prev_altitude = None
        self.steps = 0
        
    def shape_reward(self, obs: Dict, action: float, reward: float, done: bool) -> float:
        """Aplicar reward shaping para mejorar la se√±al de aprendizaje"""
        shaped_reward = reward
        
        # Obtener valores actuales
        current_alt = obs['altitude'][0]
        target_alt = obs['target_altitude'][0]
        runway_dist = obs['runway_distance'][0]
        vz = obs['vz'][0]
        
        # 1. Recompensa por reducir error de altitud
        altitude_error = abs(target_alt - current_alt)
        if self.prev_altitude_error is not None:
            if altitude_error < self.prev_altitude_error:
                shaped_reward += 2.0  # Bonificaci√≥n por mejorar
            else:
                shaped_reward -= 0.5  # Penalizaci√≥n menor por empeorar
        
        # 2. Recompensa por mantener velocidad vertical apropiada
        # Velocidad vertical √≥ptima depende de la distancia a la pista
        if runway_dist > 0.5:  # Lejos de la pista
            optimal_vz = -0.2 if current_alt > target_alt else 0.2
        else:  # Cerca de la pista
            optimal_vz = -0.1 if current_alt > 0.1 else 0.0
            
        vz_error = abs(vz - optimal_vz)
        shaped_reward += 1.0 - vz_error * 2.0
        
        # 3. Bonificaci√≥n por supervivencia
        if not done:
            shaped_reward += 0.5
            
        # 4. Penalizaci√≥n por acciones extremas
        shaped_reward -= abs(action) * 0.2
        
        # 5. Bonificaci√≥n por estar cerca del target al final
        if done and runway_dist <= 0:
            if altitude_error < 0.1:
                shaped_reward += 20.0  # Gran bonificaci√≥n por aterrizaje perfecto
            elif altitude_error < 0.2:
                shaped_reward += 10.0  # Bonificaci√≥n moderada
            elif altitude_error < 0.3:
                shaped_reward += 5.0   # Bonificaci√≥n peque√±a
        
        # Actualizar estado previo
        self.prev_altitude_error = altitude_error
        self.prev_altitude = current_alt
        self.steps += 1
        
        return shaped_reward
    
    def reset(self):
        """Reset del reward shaper"""
        self.prev_altitude_error = None
        self.prev_altitude = None
        self.steps = 0

class RewardShaperTarget30:
    """Reward shaping EXTREMO espec√≠ficamente para alcanzar -30"""
    
    def __init__(self):
        self.prev_altitude_error = None
        self.prev_altitude = None
        self.prev_action = None
        self.steps = 0
        
    def shape_reward(self, obs: Dict, action: float, reward: float, done: bool) -> float:
        """Reward shaping agresivo para alcanzar -30"""
        shaped_reward = reward
        
        # Obtener valores actuales
        current_alt = obs['altitude'][0]
        target_alt = obs['target_altitude'][0]
        runway_dist = obs['runway_distance'][0]
        vz = obs['vz'][0]
        
        altitude_error = abs(target_alt - current_alt)
        
        # MEJORA 1: Bonificaci√≥n exponencial MASIVA por precisi√≥n
        if altitude_error < 0.02:
            shaped_reward += 500.0  # MEGA BONIFICACI√ìN
        elif altitude_error < 0.05:
            shaped_reward += 200.0
        elif altitude_error < 0.1:
            shaped_reward += 100.0
        elif altitude_error < 0.15:
            shaped_reward += 50.0
        elif altitude_error < 0.2:
            shaped_reward += 25.0
            
        # MEJORA 2: Penalizaci√≥n SEVERA por errores grandes
        if altitude_error > 0.3:
            shaped_reward -= (altitude_error - 0.3) ** 2 * 1000
        elif altitude_error > 0.2:
            shaped_reward -= (altitude_error - 0.2) ** 2 * 500
            
        # MEJORA 3: Bonificaci√≥n MASIVA por mejora
        if self.prev_altitude_error is not None:
            improvement = self.prev_altitude_error - altitude_error
            shaped_reward += improvement * 500  # 5x m√°s agresivo
            
        # MEJORA 4: Bonificaci√≥n por trayectoria suave
        if self.prev_action is not None:
            action_smoothness = abs(action - self.prev_action)
            if action_smoothness < 0.1:
                shaped_reward += 50.0 * (0.1 - action_smoothness) * 10
            
        # MEJORA 5: Velocidad vertical √≥ptima con bonificaci√≥n agresiva
        if runway_dist > 0.5:
            optimal_vz = -0.3 if current_alt > target_alt else 0.3
        else:
            optimal_vz = -0.1 if current_alt > target_alt else 0.1
            
        vz_error = abs(vz - optimal_vz)
        shaped_reward += max(0, 20.0 - vz_error * 50)
        
        # MEJORA 6: JACKPOT por aterrizaje perfecto
        if done and runway_dist <= 0:
            if altitude_error < 0.01:
                shaped_reward += 2000.0  # ¬°¬°¬°JACKPOT!!!
            elif altitude_error < 0.02:
                shaped_reward += 1000.0
            elif altitude_error < 0.05:
                shaped_reward += 500.0
            elif altitude_error < 0.1:
                shaped_reward += 250.0
            elif altitude_error < 0.2:
                shaped_reward += 100.0
        
        # Actualizar estado previo
        self.prev_altitude_error = altitude_error
        self.prev_altitude = current_alt
        self.prev_action = action
        self.steps += 1
        
        return shaped_reward
    
    def reset(self):
        """Reset del reward shaper"""
        self.prev_altitude_error = None
        self.prev_altitude = None
        self.prev_action = None
        self.steps = 0

def main():
    """Funci√≥n principal para ejecutar el experimento completo"""
    
    print("="*80)
    print("üéØ PROYECTO FLAN - OPTIMIZACI√ìN EXTREMA PARA ALCANZAR RECOMPENSA -30")
    print("="*80)
    print("üìä B√öSQUEDA DE HIPERPAR√ÅMETROS AGRESIVA:")
    print("   ‚Ä¢ Q-Learning: 3 combinaciones AGRESIVAS √ó 1,500 episodios = 4,500")
    print("   ‚Ä¢ Stochastic Q-Learning: 4 combinaciones AGRESIVAS √ó 1,500 episodios = 6,000")
    print("   ‚Ä¢ Total b√∫squeda: 10,500 episodios")
    print("\nüèãÔ∏è ENTRENAMIENTO FINAL MASIVO:")
    print("   ‚Ä¢ Q-Learning final: 8,000 episodios (EXTREMO)")
    print("   ‚Ä¢ Stochastic Q-Learning final: 8,000 episodios (EXTREMO)")
    print("   ‚Ä¢ Evaluaci√≥n Q-Learning: 1,000 episodios (ROBUSTA)")
    print("   ‚Ä¢ Evaluaci√≥n Stochastic: 1,000 episodios (ROBUSTA)")
    print("   ‚Ä¢ TOTAL: 28,500 episodios (OBJETIVO: ALCANZAR -30)")
    print("\n‚è±Ô∏è  TIEMPO ESTIMADO:")
    print("   ‚Ä¢ Con CPU optimizado: ~6-8 horas")
    print("   ‚Ä¢ OBJETIVO: Recompensa consistente >= -30")
    print("\nüöÄ MEJORAS EXTREMAS APLICADAS:")
    print("   ‚Ä¢ Reward Shaping AGRESIVO (bonificaciones 10x m√°s grandes)")
    print("   ‚Ä¢ Learning Rate EXTREMO (0.7-0.9)")
    print("   ‚Ä¢ Discount Factor M√ÅXIMO (0.999)")
    print("   ‚Ä¢ Entrenamiento MASIVO (8,000 episodios finales)")
    print("   ‚Ä¢ Evaluaci√≥n EXHAUSTIVA (1,000 episodios)")
    print("="*80)
    
    # CONFIGURACI√ìN OPTIMIZADA: DescentEnv real prioritario para m√°ximo rendimiento
    with open(os.devnull, 'w') as devnull:
        with redirect_stdout(devnull), redirect_stderr(devnull):
            if BLUESKY_AVAILABLE and DescentEnv is not None:
                env = DescentEnv(render_mode=None)
                print("üöÄ USANDO DESCENTENV REAL - M√°ximo rendimiento para entrenamiento final")
            elif MOCK_AVAILABLE and MockDescentEnv is not None:
                env = MockDescentEnv(render_mode=None)
                print("üìÑ Fallback: Usando MockDescentEnv para experimento")
            else:
                raise ImportError("‚ùå Error cr√≠tico: No hay entornos disponibles")
    
    # OPTIMIZACI√ìN: Solo esquema Media para 10k episodios
    discretization_schemes = [
        DiscretizationScheme("Media", 25, 25, 25, 25, 10)
    ]
    
    results_summary = {}
    
    for scheme in discretization_schemes:
        print(f"\n{'='*50}")
        print(f"Probando esquema de discretizaci√≥n: {scheme.name}")
        print(f"Bins: Alt={scheme.altitude_bins}, Vel={scheme.velocity_bins}, "
              f"Target={scheme.target_alt_bins}, Dist={scheme.runway_dist_bins}, "
              f"Actions={scheme.action_bins}")
        print(f"{'='*50}")
        
        # OPTIMIZACI√ìN: Usar DescentEnv real incluso para hiperpar√°metros cuando sea posible
        print("\n1. Optimizando hiperpar√°metros para Q-Learning est√°ndar...")
        print(f"   ‚Ä¢ Episodios por combinaci√≥n: 800 (entrenamiento) + 100 (evaluaci√≥n)")
        
        # Usar DescentEnv real prioritario para m√°ximo rendimiento
        with open(os.devnull, 'w') as devnull:
            with redirect_stdout(devnull), redirect_stderr(devnull):
                if BLUESKY_AVAILABLE and DescentEnv is not None:
                    print("üöÄ Optimizador usando DescentEnv REAL para m√°xima precisi√≥n")
                    optimizer_env = DescentEnv(render_mode=None)
                elif MOCK_AVAILABLE and MockDescentEnv is not None:
                    print("üìÑ Optimizador usando MockDescentEnv como fallback")
                    optimizer_env = MockDescentEnv(render_mode=None)
                else:
                    raise ImportError("‚ùå No hay entornos disponibles para optimizaci√≥n")
        
        optimizer = HyperparameterOptimizer(optimizer_env, scheme)
        qlearning_results = optimizer.grid_search('qlearning')
        
        # Entrenar mejor agente Q-Learning - OPTIMIZACI√ìN TARGET -30: Entrenamiento EXTENSIVO  
        FINAL_TRAINING_EPISODES = 8000  # Entrenamiento masivo para alcanzar -30
        print(f"\n   ‚Ä¢ Entrenando mejor agente Q-Learning con {FINAL_TRAINING_EPISODES} episodios...")
        best_qlearning_agent = QLearningAgent(scheme, **qlearning_results['best_params'])
        qlearning_trainer = QLearningTrainer(env, best_qlearning_agent, scheme)
        qlearning_trainer.train(episodes=FINAL_TRAINING_EPISODES, verbose=True)
        
        # Evaluaci√≥n final robusta para TARGET -30
        FINAL_EVALUATION_EPISODES = 1000  # Evaluaci√≥n exhaustiva para confirmar -30
        print(f"\n   ‚Ä¢ Evaluando Q-Learning con {FINAL_EVALUATION_EPISODES} episodios...")
        qlearning_evaluator = PerformanceEvaluator(env, best_qlearning_agent, scheme)
        qlearning_eval = qlearning_evaluator.evaluate_multiple_episodes(num_episodes=FINAL_EVALUATION_EPISODES)
        
        # Optimizar hiperpar√°metros para Stochastic Q-Learning
        print("\n2. Optimizando hiperpar√°metros para Stochastic Q-Learning...")
        print(f"   ‚Ä¢ Episodios por combinaci√≥n: 400 (entrenamiento) + 50 (evaluaci√≥n)")
        stochastic_results = optimizer.grid_search('stochastic')
        
        # Entrenar mejor agente Stochastic Q-Learning 
        print(f"\n   ‚Ä¢ Entrenando mejor agente Stochastic Q-Learning con {FINAL_TRAINING_EPISODES} episodios...")
        best_stochastic_agent = StochasticQLearningAgent(scheme, **stochastic_results['best_params'])
        stochastic_trainer = QLearningTrainer(env, best_stochastic_agent, scheme)
        stochastic_trainer.train(episodes=FINAL_TRAINING_EPISODES, verbose=True)
        
        # Evaluar Stochastic Q-Learning
        print(f"\n   ‚Ä¢ Evaluando Stochastic Q-Learning con {FINAL_EVALUATION_EPISODES} episodios...")
        stochastic_evaluator = PerformanceEvaluator(env, best_stochastic_agent, scheme)
        stochastic_eval = stochastic_evaluator.evaluate_multiple_episodes(num_episodes=FINAL_EVALUATION_EPISODES)
        
        # Guardar modelos entrenados
        models_dir = f"models_{scheme.name.lower()}_10k"
        os.makedirs(models_dir, exist_ok=True)
        
        with open(f"{models_dir}/qlearning_agent.pkl", 'wb') as f:
            pickle.dump(best_qlearning_agent, f)
        
        with open(f"{models_dir}/stochastic_agent.pkl", 'wb') as f:
            pickle.dump(best_stochastic_agent, f)
        
        with open(f"{models_dir}/discretization.pkl", 'wb') as f:
            pickle.dump(scheme, f)
        
        print(f"Modelos guardados en {models_dir}/")
        
        # Guardar resultados - OPTIMIZACI√ìN 10K: Q-Learning + Stochastic
        results_summary[scheme.name] = {
            'qlearning': {
                'best_params': qlearning_results['best_params'],
                'best_score': qlearning_results['best_score'],
                'evaluation': qlearning_eval,
                'training_rewards': qlearning_trainer.training_rewards
            },
            'stochastic': {
                'best_params': stochastic_results['best_params'],
                'best_score': stochastic_results['best_score'],
                'evaluation': stochastic_eval,
                'training_rewards': stochastic_trainer.training_rewards
            }
        }
    
    # Guardar resultados - OPTIMIZACI√ìN 10K
    with open('flan_results_10k.json', 'w') as f:
        # Convertir numpy arrays a listas para serializaci√≥n JSON - OPTIMIZACI√ìN 10K
        serializable_results = {}
        for scheme_name, scheme_results in results_summary.items():
            serializable_results[scheme_name] = {}
            for agent_type, agent_results in scheme_results.items():
                serializable_results[scheme_name][agent_type] = {
                    'best_params': agent_results['best_params'],
                    'best_score': float(agent_results['best_score']),
                    'evaluation': {k: [float(x) for x in v] for k, v in agent_results['evaluation'].items()},
                    'training_rewards': [float(x) for x in agent_results['training_rewards']]
                }
        
        # Agregar informaci√≥n de optimizaci√≥n
        serializable_results['experiment_info'] = {
            'optimization': 'TARGET_30_EXTREME_OPTIMIZATION',
            'objective': 'Alcanzar recompensa consistente >= -30',
            'total_episodes': 28500,
            'estimated_time_hours': '6-8',
            'schemes_used': 1,
            'agents_used': ['qlearning', 'stochastic_qlearning'],
            'cpu_optimized': True,
            'hyperparameter_combinations': {
                'qlearning': 3,
                'stochastic': 4
            },
            'extreme_optimizations': [
                'RewardShaperTarget30 - bonificaciones 10x m√°s grandes',
                'Learning rates agresivos (0.7-0.9)',
                'Discount factor m√°ximo (0.999)',
                'Entrenamiento masivo (8,000 episodios finales)',
                'Evaluaci√≥n exhaustiva (1,000 episodios)'
            ]
        }
        json.dump(serializable_results, f, indent=2)
    
    # Generar reporte
    generate_report(results_summary)
    
    return results_summary

def generate_report(results_summary: Dict):
    """Genera un reporte completo de los resultados"""
    
    print("\n" + "="*80)
    print("REPORTE FINAL - PROYECTO FLAN")
    print("="*80)
    
    # Comparar esquemas de discretizaci√≥n
    print("\n1. COMPARACI√ìN DE ESQUEMAS DE DISCRETIZACI√ìN")
    print("-" * 50)
    
    for scheme_name, scheme_results in results_summary.items():
        print(f"\nEsquema: {scheme_name}")
        
        ql_score = scheme_results['qlearning']['best_score']
        stoch_score = scheme_results['stochastic']['best_score']
        
        print(f"  Q-Learning est√°ndar: {ql_score:.2f}")
        print(f"  Stochastic Q-Learning: {stoch_score:.2f}")
        print(f"  Mejora: {((stoch_score - ql_score) / abs(ql_score) * 100):.1f}%" if ql_score != 0 else "N/A")
        
        # Mejores hiperpar√°metros
        print(f"  Mejores hiperpar√°metros Q-Learning: {scheme_results['qlearning']['best_params']}")
        print(f"  Mejores hiperpar√°metros Stochastic: {scheme_results['stochastic']['best_params']}")
    
    # An√°lisis de rendimiento
    print("\n2. AN√ÅLISIS DE RENDIMIENTO")
    print("-" * 50)
    
    best_scheme = max(results_summary.keys(), 
                     key=lambda x: results_summary[x]['stochastic']['best_score'])
    
    print(f"Mejor esquema de discretizaci√≥n: {best_scheme}")
    
    best_results = results_summary[best_scheme]['stochastic']['evaluation']
    
    print(f"Recompensa promedio: {np.mean(best_results['total_rewards']):.2f} ¬± {np.std(best_results['total_rewards']):.2f}")
    print(f"Tiempo de supervivencia promedio: {np.mean(best_results['survival_times']):.1f} pasos")
    print(f"Error de altitud promedio: {np.mean(best_results['altitude_errors']):.3f}")
    
    # Generar gr√°ficos
    generate_plots(results_summary)

def generate_plots(results_summary: Dict):
    """Genera gr√°ficos de los resultados"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Comparaci√≥n de scores por esquema
    schemes = list(results_summary.keys())
    ql_scores = [results_summary[s]['qlearning']['best_score'] for s in schemes]
    stoch_scores = [results_summary[s]['stochastic']['best_score'] for s in schemes]
    
    x = np.arange(len(schemes))
    width = 0.35
    
    axes[0, 0].bar(x - width/2, ql_scores, width, label='Q-Learning Est√°ndar', alpha=0.8)
    axes[0, 0].bar(x + width/2, stoch_scores, width, label='Stochastic Q-Learning', alpha=0.8)
    axes[0, 0].set_xlabel('Esquema de Discretizaci√≥n')
    axes[0, 0].set_ylabel('Score Promedio')
    axes[0, 0].set_title('Comparaci√≥n de Rendimiento por Esquema')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(schemes)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Curvas de aprendizaje para el mejor esquema
    best_scheme = max(results_summary.keys(), 
                     key=lambda x: results_summary[x]['stochastic']['best_score'])
    
    ql_rewards = results_summary[best_scheme]['qlearning']['training_rewards']
    stoch_rewards = results_summary[best_scheme]['stochastic']['training_rewards']
    
    # Promedio m√≥vil
    window = 50
    ql_smooth = np.convolve(ql_rewards, np.ones(window)/window, mode='valid')
    stoch_smooth = np.convolve(stoch_rewards, np.ones(window)/window, mode='valid')
    
    axes[0, 1].plot(ql_smooth, label='Q-Learning Est√°ndar', alpha=0.8)
    axes[0, 1].plot(stoch_smooth, label='Stochastic Q-Learning', alpha=0.8)
    axes[0, 1].set_xlabel('Episodio')
    axes[0, 1].set_ylabel('Recompensa Promedio (ventana m√≥vil)')
    axes[0, 1].set_title(f'Curvas de Aprendizaje - {best_scheme}')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Distribuci√≥n de recompensas finales
    best_results = results_summary[best_scheme]['stochastic']['evaluation']
    
    axes[1, 0].hist(best_results['total_rewards'], bins=20, alpha=0.7, edgecolor='black')
    axes[1, 0].set_xlabel('Recompensa Total')
    axes[1, 0].set_ylabel('Frecuencia')
    axes[1, 0].set_title(f'Distribuci√≥n de Recompensas - {best_scheme}')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Comparaci√≥n de m√©tricas
    metrics = ['total_rewards', 'survival_times', 'altitude_errors']
    metric_names = ['Recompensa Total', 'Tiempo de Supervivencia', 'Error de Altitud']
    
    ql_means = [np.mean(results_summary[best_scheme]['qlearning']['evaluation'][m]) for m in metrics]
    stoch_means = [np.mean(results_summary[best_scheme]['stochastic']['evaluation'][m]) for m in metrics]
    
    x = np.arange(len(metrics))
    width = 0.35
    
    axes[1, 1].bar(x - width/2, ql_means, width, label='Q-Learning Est√°ndar', alpha=0.8)
    axes[1, 1].bar(x + width/2, stoch_means, width, label='Stochastic Q-Learning', alpha=0.8)
    axes[1, 1].set_xlabel('M√©trica')
    axes[1, 1].set_ylabel('Valor Promedio')
    axes[1, 1].set_title(f'Comparaci√≥n de M√©tricas - {best_scheme}')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(metric_names, rotation=45)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('flan_results.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    results = main()