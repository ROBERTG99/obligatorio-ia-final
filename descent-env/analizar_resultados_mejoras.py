#!/usr/bin/env python3
"""
An√°lisis detallado de resultados y estrategias para alcanzar recompensa -30
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def cargar_y_analizar_resultados():
    """Carga y analiza los resultados del experimento"""
    
    print("="*80)
    print("üìä AN√ÅLISIS DETALLADO DE RESULTADOS - PROYECTO FLAN")
    print("="*80)
    
    try:
        with open('flan_results_10k.json', 'r') as f:
            results = json.load(f)
        
        # Extraer datos
        scheme_results = results['Media']
        qlearning_data = scheme_results['qlearning']
        stochastic_data = scheme_results['stochastic']
        
        print("‚úÖ Resultados cargados exitosamente")
        
        return qlearning_data, stochastic_data, results
        
    except FileNotFoundError:
        print("‚ùå Error: No se encontr√≥ el archivo flan_results_10k.json")
        return None, None, None

def analizar_rendimiento_actual(qlearning_data, stochastic_data):
    """Analiza el rendimiento actual en detalle"""
    
    print("\n" + "="*60)
    print("üîç AN√ÅLISIS DE RENDIMIENTO ACTUAL")
    print("="*60)
    
    # Datos Q-Learning
    ql_rewards = np.array(qlearning_data['evaluation']['total_rewards'])
    ql_best_score = qlearning_data['best_score']
    
    # Datos Stochastic Q-Learning  
    stoch_rewards = np.array(stochastic_data['evaluation']['total_rewards'])
    stoch_best_score = stochastic_data['best_score']
    
    print(f"ü§ñ Q-LEARNING EST√ÅNDAR:")
    print(f"   ‚Ä¢ Score promedio b√∫squeda: {ql_best_score:.2f}")
    print(f"   ‚Ä¢ Recompensa evaluaci√≥n: {np.mean(ql_rewards):.2f} ¬± {np.std(ql_rewards):.2f}")
    print(f"   ‚Ä¢ Rango: [{np.min(ql_rewards):.2f}, {np.max(ql_rewards):.2f}]")
    print(f"   ‚Ä¢ Mejor episodio: {np.max(ql_rewards):.2f}")
    print(f"   ‚Ä¢ Percentil 90: {np.percentile(ql_rewards, 90):.2f}")
    
    print(f"\nüé≤ STOCHASTIC Q-LEARNING:")
    print(f"   ‚Ä¢ Score promedio b√∫squeda: {stoch_best_score:.2f}")
    print(f"   ‚Ä¢ Recompensa evaluaci√≥n: {np.mean(stoch_rewards):.2f} ¬± {np.std(stoch_rewards):.2f}")
    print(f"   ‚Ä¢ Rango: [{np.min(stoch_rewards):.2f}, {np.max(stoch_rewards):.2f}]")
    print(f"   ‚Ä¢ Mejor episodio: {np.max(stoch_rewards):.2f}")
    print(f"   ‚Ä¢ Percentil 90: {np.percentile(stoch_rewards, 90):.2f}")
    
    # An√°lisis comparativo
    mejora_absoluta = np.mean(stoch_rewards) - np.mean(ql_rewards)
    print(f"\nüìà COMPARACI√ìN:")
    print(f"   ‚Ä¢ Mejora Stochastic vs Q-Learning: {mejora_absoluta:.2f}")
    print(f"   ‚Ä¢ Mejora relativa: {mejora_absoluta/abs(np.mean(ql_rewards))*100:.1f}%")
    
    # Distancia al objetivo
    objetivo = -30
    distancia_ql = abs(objetivo - np.mean(ql_rewards))
    distancia_stoch = abs(objetivo - np.mean(stoch_rewards))
    
    print(f"\nüéØ DISTANCIA AL OBJETIVO (-30):")
    print(f"   ‚Ä¢ Q-Learning: {distancia_ql:.2f} puntos")
    print(f"   ‚Ä¢ Stochastic: {distancia_stoch:.2f} puntos")
    print(f"   ‚Ä¢ Mejora necesaria: {distancia_stoch:.2f} puntos (~{distancia_stoch/abs(np.mean(stoch_rewards))*100:.1f}%)")
    
    return ql_rewards, stoch_rewards, objetivo

def identificar_problemas_clave(qlearning_data, stochastic_data):
    """Identifica los principales problemas que limitan el rendimiento"""
    
    print("\n" + "="*60)
    print("üö® IDENTIFICACI√ìN DE PROBLEMAS CLAVE")
    print("="*60)
    
    # Analizar hiperpar√°metros
    ql_params = qlearning_data['best_params']
    stoch_params = stochastic_data['best_params']
    
    print("üîß HIPERPAR√ÅMETROS ACTUALES:")
    print(f"   Q-Learning: {ql_params}")
    print(f"   Stochastic: {stoch_params}")
    
    problemas_identificados = []
    
    # Problema 1: Learning rate bajo
    if ql_params['learning_rate'] <= 0.3:
        problemas_identificados.append({
            'problema': 'Learning rate conservador',
            'descripcion': f"LR actual: {ql_params['learning_rate']}. Puede ser muy lento para convergencia √≥ptima.",
            'impacto': 'Alto',
            'solucion': 'Probar LR m√°s altos (0.5-0.8) con decay adaptativo'
        })
    
    # Problema 2: Epsilon muy bajo
    if ql_params['epsilon'] <= 0.2:
        problemas_identificados.append({
            'problema': 'Exploraci√≥n insuficiente',
            'descripcion': f"Epsilon: {ql_params['epsilon']}. Poca exploraci√≥n puede llevar a convergencia prematura.",
            'impacto': 'Alto',
            'solucion': 'Implementar epsilon decay din√°mico (inicio 0.9, final 0.01)'
        })
    
    # Problema 3: Discretizaci√≥n limitada
    problemas_identificados.append({
        'problema': 'Discretizaci√≥n Media puede ser sub√≥ptima',
        'descripcion': '25x25x25x25x10 puede no capturar suficiente detalle para control fino.',
        'impacto': 'Medio',
        'solucion': 'Probar discretizaci√≥n Fina (50x50x50x50x20) para mayor precisi√≥n'
    })
    
    # Problema 4: Reward shaping insuficiente
    problemas_identificados.append({
        'problema': 'Reward shaping b√°sico',
        'descripcion': 'El reward shaping actual puede no estar optimizado para el objetivo espec√≠fico.',
        'impacto': 'Alto',
        'solucion': 'Mejorar reward shaping con bonificaciones m√°s agresivas por precisi√≥n'
    })
    
    print(f"\nüö® PROBLEMAS IDENTIFICADOS ({len(problemas_identificados)}):")
    for i, problema in enumerate(problemas_identificados, 1):
        print(f"\n{i}. {problema['problema']} (Impacto: {problema['impacto']})")
        print(f"   üí° {problema['descripcion']}")
        print(f"   üîß Soluci√≥n: {problema['solucion']}")
    
    return problemas_identificados

def proponer_estrategias_mejora():
    """Propone estrategias espec√≠ficas para alcanzar -30"""
    
    print("\n" + "="*60)
    print("üöÄ ESTRATEGIAS PARA ALCANZAR RECOMPENSA -30")
    print("="*60)
    
    estrategias = [
        {
            'nombre': 'OPTIMIZACI√ìN AGRESIVA DE HIPERPAR√ÅMETROS',
            'descripcion': 'Expandir grid search con par√°metros m√°s agresivos',
            'mejora_esperada': '8-12 puntos',
            'implementacion': [
                'learning_rate: [0.5, 0.7, 0.8] - Aprendizaje m√°s r√°pido',
                'epsilon decay: din√°mico 0.9 ‚Üí 0.01 - Mejor exploraci√≥n inicial',
                'discount_factor: [0.995, 0.999] - Mayor peso futuro'
            ],
            'esfuerzo': 'Bajo',
            'tiempo': '2 horas'
        },
        {
            'nombre': 'REWARD SHAPING AVANZADO',
            'descripcion': 'Redise√±ar funci√≥n de recompensa para incentivos m√°s fuertes',
            'mejora_esperada': '10-15 puntos',
            'implementacion': [
                'Bonificaci√≥n exponencial por precisi√≥n de altitud',
                'Penalizaci√≥n cuadr√°tica por desviaciones grandes',
                'Premio por trayectorias suaves'
            ],
            'esfuerzo': 'Medio',
            'tiempo': '3 horas'
        }
    ]
    
    print("üìã ESTRATEGIAS PROPUESTAS:")
    
    mejora_total_min = 0
    mejora_total_max = 0
    
    for i, estrategia in enumerate(estrategias, 1):
        print(f"\n{i}. {estrategia['nombre']}")
        print(f"   üìà Mejora esperada: {estrategia['mejora_esperada']}")
        print(f"   üîß Esfuerzo: {estrategia['esfuerzo']} | ‚è±Ô∏è Tiempo: {estrategia['tiempo']}")
        print(f"   üí° {estrategia['descripcion']}")
        print("   üõ†Ô∏è Implementaci√≥n:")
        for item in estrategia['implementacion']:
            print(f"      ‚Ä¢ {item}")
    
    return estrategias

def main():
    """Funci√≥n principal de an√°lisis"""
    
    print("üéØ AN√ÅLISIS PARA ALCANZAR RECOMPENSA -30")
    
    # Cargar y analizar resultados
    qlearning_data, stochastic_data, results = cargar_y_analizar_resultados()
    
    if qlearning_data is None:
        print("‚ùå No se pudieron cargar los resultados")
        return
    
    # An√°lisis detallado
    analizar_rendimiento_actual(qlearning_data, stochastic_data)
    
    # Identificar problemas
    identificar_problemas_clave(qlearning_data, stochastic_data)
    
    # Proponer estrategias
    proponer_estrategias_mejora()
    
    print(f"\n" + "="*80)
    print("‚úÖ AN√ÅLISIS COMPLETO TERMINADO")
    print("="*80)

if __name__ == "__main__":
    main() 