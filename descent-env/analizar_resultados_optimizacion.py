#!/usr/bin/env python3
"""
üéØ ANALIZADOR AVANZADO DE RESULTADOS FLAN
Analiza el JSON de resultados y sugiere mejoras espec√≠ficas para alcanzar -25
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
from typing import Dict, List, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

class AnalizadorResultadosFLAN:
    """Analizador avanzado para optimizar hacia recompensa -25"""
    
    def __init__(self, json_path: str):
        self.json_path = json_path
        self.resultados = self.cargar_resultados()
        self.objetivo_target = -25
        self.umbral_critico = -35  # Umbral m√≠nimo aceptable
        
    def cargar_resultados(self) -> Dict:
        """Carga los resultados del JSON"""
        try:
            with open(self.json_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Archivo {self.json_path} no encontrado. Creando an√°lisis vac√≠o.")
            return {}
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è Error al leer {self.json_path}. Puede estar incompleto.")
            return {}
    
    def analizar_rendimiento_actual(self) -> Dict[str, Any]:
        """Analiza el rendimiento actual y identifica problemas cr√≠ticos"""
        analisis = {
            'problemas_criticos': [],
            'problemas_moderados': [],
            'puntos_fuertes': [],
            'metricas_clave': {},
            'distancia_objetivo': {}
        }
        
        if not self.resultados:
            analisis['problemas_criticos'].append("No hay resultados disponibles para analizar")
            return analisis
        
        # Analizar cada esquema y agente
        for scheme_name, scheme_data in self.resultados.items():
            if scheme_name == 'experiment_info':
                continue
                
            for agent_type, agent_data in scheme_data.items():
                key = f"{scheme_name}_{agent_type}"
                
                # Obtener m√©tricas
                if 'evaluation' in agent_data:
                    rewards = agent_data['evaluation']['total_rewards']
                    steps = agent_data['evaluation']['survival_times']
                    errors = agent_data['evaluation']['altitude_errors']
                    
                    media_reward = np.mean(rewards)
                    std_reward = np.std(rewards)
                    media_steps = np.mean(steps)
                    media_error = np.mean(errors)
                    
                    analisis['metricas_clave'][key] = {
                        'media_reward': media_reward,
                        'std_reward': std_reward,
                        'media_steps': media_steps,
                        'media_error': media_error,
                        'min_reward': np.min(rewards),
                        'max_reward': np.max(rewards),
                        'q25_reward': np.percentile(rewards, 25),
                        'q75_reward': np.percentile(rewards, 75)
                    }
                    
                    # Calcular distancia al objetivo
                    distancia = abs(media_reward - self.objetivo_target)
                    analisis['distancia_objetivo'][key] = {
                        'distancia_absoluta': distancia,
                        'mejora_necesaria': self.objetivo_target - media_reward,
                        'porcentaje_mejora': (distancia / abs(media_reward)) * 100 if media_reward != 0 else float('inf')
                    }
                    
                    # Identificar problemas cr√≠ticos
                    if media_reward < -60:
                        analisis['problemas_criticos'].append(
                            f"{key}: Recompensa muy baja ({media_reward:.1f}), necesita +{self.objetivo_target - media_reward:.1f} puntos"
                        )
                    elif media_reward < -40:
                        analisis['problemas_moderados'].append(
                            f"{key}: Recompensa mejorable ({media_reward:.1f}), necesita +{self.objetivo_target - media_reward:.1f} puntos"
                        )
                    else:
                        analisis['puntos_fuertes'].append(
                            f"{key}: Recompensa prometedora ({media_reward:.1f})"
                        )
                    
                    if std_reward > 15:
                        analisis['problemas_criticos'].append(
                            f"{key}: Alta variabilidad (¬±{std_reward:.1f}), inconsistente"
                        )
                    elif std_reward > 10:
                        analisis['problemas_moderados'].append(
                            f"{key}: Variabilidad moderada (¬±{std_reward:.1f})"
                        )
                    
                    if media_steps < 100:
                        analisis['problemas_criticos'].append(
                            f"{key}: Supervivencia muy baja ({media_steps:.1f} pasos)"
                        )
                    elif media_steps < 200:
                        analisis['problemas_moderados'].append(
                            f"{key}: Supervivencia mejorable ({media_steps:.1f} pasos)"
                        )
                    else:
                        analisis['puntos_fuertes'].append(
                            f"{key}: Buena supervivencia ({media_steps:.1f} pasos)"
                        )
        
        return analisis
    
    def sugerir_mejoras_reward_shaping(self, analisis: Dict) -> List[str]:
        """Sugiere mejoras espec√≠ficas en reward shaping"""
        mejoras = []
        
        # Analizar problemas de supervivencia
        supervivencia_baja = any('Supervivencia' in p for p in analisis['problemas_criticos'])
        if supervivencia_baja:
            mejoras.extend([
                "üî¥ CR√çTICO - Reward Shaping para Supervivencia:",
                "  ‚Ä¢ Aumentar bonificaci√≥n por paso: +5 ‚Üí +10 por supervivencia",
                "  ‚Ä¢ A√±adir bonificaci√≥n exponencial: +1*step^1.1 por longevidad",
                "  ‚Ä¢ Penalizaci√≥n menor por muerte temprana: -50*max(0, 200-steps)",
                "  ‚Ä¢ Bonificaci√≥n por hitos: +50 puntos cada 100 pasos"
            ])
        
        # Analizar problemas de precisi√≥n
        for key, metricas in analisis['metricas_clave'].items():
            if metricas['media_error'] > 0.1:
                mejoras.extend([
                    f"üî¥ CR√çTICO - Reward Shaping para Precisi√≥n ({key}):",
                    "  ‚Ä¢ Bonificaci√≥n exponencial por precisi√≥n: +1000*(0.1-error)^2",
                    "  ‚Ä¢ Penalizaci√≥n suave por error: -100*error^1.5",
                    "  ‚Ä¢ Bonificaci√≥n por mejora: +20 si error < prev_error"
                ])
        
        # Analizar problemas de variabilidad
        alta_variabilidad = any('variabilidad' in p.lower() for p in analisis['problemas_criticos'])
        if alta_variabilidad:
            mejoras.extend([
                "üî¥ CR√çTICO - Reward Shaping para Consistencia:",
                "  ‚Ä¢ Reward smoothing: shaped_reward = 0.7*shaped + 0.3*prev_shaped",
                "  ‚Ä¢ Penalizaci√≥n por acciones err√°ticas: -10*abs(action-prev_action)",
                "  ‚Ä¢ Bonificaci√≥n por trayectoria suave: +5 si smooth_trajectory"
            ])
        
        return mejoras
    
    def sugerir_mejoras_hiperparametros(self, analisis: Dict) -> List[str]:
        """Sugiere ajustes de hiperpar√°metros"""
        mejoras = []
        
        # Analizar convergencia
        for key, metricas in analisis['metricas_clave'].items():
            if metricas['media_reward'] < -50:
                mejoras.extend([
                    f"üî¥ CR√çTICO - Hiperpar√°metros para Convergencia ({key}):",
                    "  ‚Ä¢ Learning Rate m√°s agresivo: 0.5 ‚Üí 0.8",
                    "  ‚Ä¢ Epsilon inicial MUY alto: 0.9 (exploraci√≥n masiva)",
                    "  ‚Ä¢ Epsilon decay ULTRA lento: 0.99995",
                    "  ‚Ä¢ Discount factor m√°ximo: 0.9999"
                ])
            elif metricas['media_reward'] < -35:
                mejoras.extend([
                    f"üü° MODERADO - Hiperpar√°metros para Mejora ({key}):",
                    "  ‚Ä¢ Learning Rate aumentado: actual ‚Üí +0.2",
                    "  ‚Ä¢ Epsilon m√°s exploratorio: +0.2 inicial",
                    "  ‚Ä¢ Decay m√°s lento: actual*0.5"
                ])
        
        # Analizar estabilidad
        for key, metricas in analisis['metricas_clave'].items():
            if metricas['std_reward'] > 15:
                mejoras.extend([
                    f"üî¥ CR√çTICO - Hiperpar√°metros para Estabilidad ({key}):",
                    "  ‚Ä¢ Learning Rate m√°s conservador para convergencia final",
                    "  ‚Ä¢ Epsilon m√≠nimo mayor: 0.05 ‚Üí 0.1 (mantener exploraci√≥n)",
                    "  ‚Ä¢ Usar experience replay con priorizaci√≥n"
                ])
        
        return mejoras
    
    def sugerir_mejoras_arquitectura(self, analisis: Dict) -> List[str]:
        """Sugiere cambios arquitecturales"""
        mejoras = []
        
        # Problemas severos requieren cambios dr√°sticos
        problemas_severos = len(analisis['problemas_criticos']) > 2
        if problemas_severos:
            mejoras.extend([
                "üî¥ CR√çTICO - Cambios Arquitecturales Dr√°sticos:",
                "  ‚Ä¢ Aumentar discretizaci√≥n: 40√ó30√ó30√ó30√ó20 ‚Üí 50√ó40√ó40√ó40√ó25",
                "  ‚Ä¢ Implementar curriculum learning: empezar con target -50, luego -35, luego -25",
                "  ‚Ä¢ Usar ensemble de agentes: combinar mejores pol√≠ticas",
                "  ‚Ä¢ Implementar prioritized experience replay",
                "  ‚Ä¢ A√±adir meta-learning: aprender a aprender m√°s r√°pido"
            ])
        
        # Problemas de supervivencia
        supervivencia_critica = any('muy baja' in p for p in analisis['problemas_criticos'])
        if supervivencia_critica:
            mejoras.extend([
                "üî¥ CR√çTICO - Arquitectura para Supervivencia:",
                "  ‚Ä¢ Aumentar l√≠mite de pasos: 1000 ‚Üí 2000",
                "  ‚Ä¢ Implementar early stopping m√°s agresivo por supervivencia",
                "  ‚Ä¢ Usar reward shaping adaptativo basado en performance",
                "  ‚Ä¢ Pre-entrenar con target de supervivencia antes que precisi√≥n"
            ])
        
        return mejoras
    
    def analizar_convergencia(self) -> Dict[str, Any]:
        """Analiza la convergencia del entrenamiento"""
        convergencia = {}
        
        for scheme_name, scheme_data in self.resultados.items():
            if scheme_name == 'experiment_info':
                continue
                
            for agent_type, agent_data in scheme_data.items():
                if 'training_rewards' not in agent_data:
                    continue
                    
                rewards = agent_data['training_rewards']
                key = f"{scheme_name}_{agent_type}"
                
                # Analizar tendencia
                if len(rewards) > 100:
                    inicio = np.mean(rewards[:100])
                    final = np.mean(rewards[-100:])
                    mejora = final - inicio
                    
                    # Analizar ventanas de convergencia
                    ventanas = [rewards[i:i+1000] for i in range(0, len(rewards)-1000, 1000)]
                    medias_ventanas = [np.mean(v) for v in ventanas]
                    
                    convergencia[key] = {
                        'mejora_total': mejora,
                        'mejora_porcentual': (mejora / abs(inicio)) * 100 if inicio != 0 else 0,
                        'convergio': mejora > 0,
                        'plateando': len(medias_ventanas) > 2 and np.std(medias_ventanas[-3:]) < 2,
                        'medias_ventanas': medias_ventanas,
                        'reward_inicial': inicio,
                        'reward_final': final
                    }
        
        return convergencia
    
    def generar_recomendaciones_prioritarias(self) -> List[str]:
        """Genera recomendaciones priorizadas por impacto"""
        analisis = self.analizar_rendimiento_actual()
        convergencia = self.analizar_convergencia()
        
        recomendaciones = ["üéØ RECOMENDACIONES PRIORITARIAS PARA ALCANZAR -25:"]
        recomendaciones.append("="*60)
        
        # Prioridad 1: Problemas cr√≠ticos de reward
        distancias = analisis['distancia_objetivo']
        if distancias:
            peor_caso = max(distancias.items(), key=lambda x: x[1]['distancia_absoluta'])
            mejor_caso = min(distancias.items(), key=lambda x: x[1]['distancia_absoluta'])
            
            recomendaciones.extend([
                f"\nüî¥ PRIORIDAD 1 - BREAKTHROUGH CR√çTICO:",
                f"  ‚Ä¢ Peor caso: {peor_caso[0]} necesita +{peor_caso[1]['mejora_necesaria']:.1f} puntos",
                f"  ‚Ä¢ Mejor caso: {mejor_caso[0]} necesita +{mejor_caso[1]['mejora_necesaria']:.1f} puntos",
                f"  ‚Ä¢ ACCI√ìN: Implementar RewardShaperBreakthrough con bonificaciones 5x m√°s altas"
            ])
        
        # Prioridad 2: Problemas de convergencia
        no_converge = [k for k, v in convergencia.items() if not v.get('convergio', False)]
        if no_converge:
            recomendaciones.extend([
                f"\nüü° PRIORIDAD 2 - CONVERGENCIA:",
                f"  ‚Ä¢ Agentes sin convergencia: {', '.join(no_converge)}",
                f"  ‚Ä¢ ACCI√ìN: Aumentar learning rate a 0.9, episodios a 100k"
            ])
        
        # Prioridad 3: Problemas de supervivencia
        supervivencia = any('Supervivencia muy baja' in p for p in analisis['problemas_criticos'])
        if supervivencia:
            recomendaciones.extend([
                f"\nüî¥ PRIORIDAD 3 - SUPERVIVENCIA CR√çTICA:",
                f"  ‚Ä¢ ACCI√ìN INMEDIATA: +20 puntos por paso de supervivencia",
                f"  ‚Ä¢ L√≠mite de pasos: 1000 ‚Üí 3000",
                f"  ‚Ä¢ Pre-entrenamiento solo para supervivencia (ignora precisi√≥n)"
            ])
        
        # Sugerencias espec√≠ficas
        mejoras_rs = self.sugerir_mejoras_reward_shaping(analisis)
        mejoras_hp = self.sugerir_mejoras_hiperparametros(analisis)
        mejoras_arch = self.sugerir_mejoras_arquitectura(analisis)
        
        if mejoras_rs:
            recomendaciones.extend(["\nüìà MEJORAS REWARD SHAPING:"] + mejoras_rs)
        if mejoras_hp:
            recomendaciones.extend(["\n‚öôÔ∏è MEJORAS HIPERPAR√ÅMETROS:"] + mejoras_hp)
        if mejoras_arch:
            recomendaciones.extend(["\nüèóÔ∏è MEJORAS ARQUITECTURA:"] + mejoras_arch)
        
        return recomendaciones
    
    def generar_codigo_mejoras(self) -> str:
        """Genera c√≥digo Python con las mejoras sugeridas"""
        analisis = self.analizar_rendimiento_actual()
        
        codigo = '''
# üöÄ MEJORAS AUTOM√ÅTICAS SUGERIDAS PARA ALCANZAR -25

class RewardShaperBreakthrough:
    """Reward shaper ULTRA AGRESIVO para breakthrough -70 ‚Üí -25"""
    
    def __init__(self):
        self.step_count = 0
        self.best_error = float('inf')
        self.prev_action = 0
        
    def shape_reward(self, obs, action, reward, done):
        shaped = reward
        
        altitude_error = abs(obs['target_altitude'][0] - obs['altitude'][0])
        self.step_count += 1
        
        # SUPERVIVENCIA MASIVA (problema cr√≠tico identificado)
        if not done:
            shaped += 20.0  # +20 por cada paso (vs +5 anterior)
            
        # BONIFICACI√ìN EXPONENCIAL POR SUPERVIVENCIA
        if self.step_count > 200:
            shaped += self.step_count * 0.1  # Crece con el tiempo
            
        # JACKPOT MEGA por precisi√≥n
        if altitude_error < 0.01:
            shaped += 10000.0  # 10x m√°s que antes
        elif altitude_error < 0.05:
            shaped += 5000.0
        elif altitude_error < 0.1:
            shaped += 2000.0
            
        # PENALIZACI√ìN C√öBICA por errores grandes
        if altitude_error > 0.2:
            shaped -= (altitude_error ** 3) * 5000
            
        # CONSISTENCIA (problema variabilidad identificado)
        action_consistency = abs(action - self.prev_action)
        if action_consistency < 0.1:
            shaped += 50.0
        else:
            shaped -= action_consistency * 100
            
        self.prev_action = action
        return shaped
    
    def reset(self):
        self.step_count = 0
        self.best_error = float('inf')
        self.prev_action = 0

# HIPERPAR√ÅMETROS BREAKTHROUGH
BREAKTHROUGH_CONFIG = {
    'learning_rate': 0.9,        # Ultra agresivo
    'epsilon': 0.95,             # Exploraci√≥n m√°xima
    'epsilon_decay': 0.99999,    # Decay ultra lento
    'epsilon_min': 0.1,          # Nunca dejar de explorar
    'discount_factor': 0.9999,   # M√°ximo peso al futuro
    'episodes': 150000,          # 1.5x m√°s episodios
    'step_limit': 3000,          # Triple l√≠mite de pasos
}

# EARLY STOPPING ADAPTATIVO
def early_stopping_breakthrough(rewards, min_episodes=2000):
    if len(rewards) < min_episodes:
        return False
    
    ventana = 1000
    media_reciente = np.mean(rewards[-ventana:])
    
    # Objetivo progresivo: -50 ‚Üí -35 ‚Üí -25
    if len(rewards) < 50000 and media_reciente > -50:
        return True
    elif len(rewards) < 100000 and media_reciente > -35:
        return True
    elif media_reciente > -25:
        return True
    
    return False
'''
        
        return codigo
    
    def crear_visualizaciones(self):
        """Crea visualizaciones avanzadas del an√°lisis"""
        if not self.resultados:
            print("‚ùå No hay datos para visualizar")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('üéØ AN√ÅLISIS AVANZADO FLAN - OPTIMIZACI√ìN HACIA -25', fontsize=16, fontweight='bold')
        
        # Gr√°fico 1: Distancia al objetivo
        distancias = self.analizar_rendimiento_actual()['distancia_objetivo']
        if distancias:
            keys = list(distancias.keys())
            mejoras_necesarias = [distancias[k]['mejora_necesaria'] for k in keys]
            
            colors = ['red' if m > 30 else 'orange' if m > 15 else 'green' for m in mejoras_necesarias]
            bars = axes[0,0].bar(range(len(keys)), mejoras_necesarias, color=colors, alpha=0.7)
            axes[0,0].axhline(y=0, color='green', linestyle='--', alpha=0.7, label='Objetivo -25')
            axes[0,0].set_title('Mejora Necesaria para Alcanzar -25')
            axes[0,0].set_ylabel('Puntos de Mejora Necesarios')
            axes[0,0].set_xticks(range(len(keys)))
            axes[0,0].set_xticklabels(keys, rotation=45)
            
            # A√±adir valores en las barras
            for i, bar in enumerate(bars):
                height = bar.get_height()
                axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                             f'{height:.1f}', ha='center', va='bottom')
        
        # Gr√°fico 2: Convergencia temporal
        convergencia = self.analizar_convergencia()
        for i, (key, data) in enumerate(convergencia.items()):
            if 'medias_ventanas' in data and data['medias_ventanas']:
                axes[0,1].plot(data['medias_ventanas'], label=key, marker='o', alpha=0.7)
        
        axes[0,1].axhline(y=-25, color='green', linestyle='--', alpha=0.7, label='Objetivo -25')
        axes[0,1].axhline(y=-35, color='orange', linestyle='--', alpha=0.7, label='Umbral Cr√≠tico -35')
        axes[0,1].set_title('Convergencia por Ventanas de 1000 Episodios')
        axes[0,1].set_ylabel('Recompensa Media')
        axes[0,1].set_xlabel('Ventana de Entrenamiento')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # Gr√°fico 3: Distribuci√≥n de recompensas
        all_rewards = []
        labels = []
        for scheme_name, scheme_data in self.resultados.items():
            if scheme_name == 'experiment_info':
                continue
            for agent_type, agent_data in scheme_data.items():
                if 'evaluation' in agent_data:
                    all_rewards.append(agent_data['evaluation']['total_rewards'])
                    labels.append(f"{scheme_name}_{agent_type}")
        
        if all_rewards:
            axes[0,2].boxplot(all_rewards, labels=labels)
            axes[0,2].axhline(y=-25, color='green', linestyle='--', alpha=0.7, label='Objetivo -25')
            axes[0,2].axhline(y=-35, color='orange', linestyle='--', alpha=0.7, label='Cr√≠tico -35')
            axes[0,2].set_title('Distribuci√≥n de Recompensas Finales')
            axes[0,2].set_ylabel('Recompensa')
            axes[0,2].tick_params(axis='x', rotation=45)
            axes[0,2].legend()
        
        # Gr√°fico 4: Supervivencia vs Recompensa
        analisis = self.analizar_rendimiento_actual()
        if analisis['metricas_clave']:
            survival_times = [m['media_steps'] for m in analisis['metricas_clave'].values()]
            rewards = [m['media_reward'] for m in analisis['metricas_clave'].values()]
            keys = list(analisis['metricas_clave'].keys())
            
            scatter = axes[1,0].scatter(survival_times, rewards, c=range(len(keys)), 
                                      s=100, alpha=0.7, cmap='viridis')
            axes[1,0].axhline(y=-25, color='green', linestyle='--', alpha=0.7)
            axes[1,0].set_xlabel('Tiempo de Supervivencia Promedio')
            axes[1,0].set_ylabel('Recompensa Promedio')
            axes[1,0].set_title('Correlaci√≥n Supervivencia-Recompensa')
            
            # A√±adir etiquetas
            for i, key in enumerate(keys):
                axes[1,0].annotate(key, (survival_times[i], rewards[i]), 
                                 xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        # Gr√°fico 5: Heatmap de performance
        if analisis['metricas_clave']:
            metrics_matrix = []
            metric_names = ['media_reward', 'std_reward', 'media_steps', 'media_error']
            agent_names = list(analisis['metricas_clave'].keys())
            
            for agent in agent_names:
                metrics_matrix.append([
                    analisis['metricas_clave'][agent]['media_reward'],
                    -analisis['metricas_clave'][agent]['std_reward'],  # Negativo porque menor es mejor
                    analisis['metricas_clave'][agent]['media_steps'],
                    -analisis['metricas_clave'][agent]['media_error']   # Negativo porque menor es mejor
                ])
            
            if metrics_matrix:
                im = axes[1,1].imshow(metrics_matrix, cmap='RdYlGn', aspect='auto')
                axes[1,1].set_xticks(range(len(metric_names)))
                axes[1,1].set_xticklabels(['Recompensa', 'Consistencia', 'Supervivencia', 'Precisi√≥n'])
                axes[1,1].set_yticks(range(len(agent_names)))
                axes[1,1].set_yticklabels(agent_names)
                axes[1,1].set_title('Mapa de Calor de Performance')
                plt.colorbar(im, ax=axes[1,1])
        
        # Gr√°fico 6: Recomendaciones prioritarias (texto)
        recomendaciones = self.generar_recomendaciones_prioritarias()
        texto_rec = '\n'.join(recomendaciones[:15])  # Mostrar solo las primeras 15 l√≠neas
        axes[1,2].text(0.05, 0.95, texto_rec, transform=axes[1,2].transAxes, 
                      fontsize=8, verticalalignment='top', fontfamily='monospace')
        axes[1,2].set_title('Recomendaciones Prioritarias')
        axes[1,2].axis('off')
        
        plt.tight_layout()
        plt.savefig('analisis_optimizacion_flan.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def generar_reporte_completo(self):
        """Genera un reporte completo del an√°lisis"""
        print("üéØ ANALIZADOR AVANZADO FLAN - REPORTE COMPLETO")
        print("="*80)
        
        # Informaci√≥n b√°sica
        if 'experiment_info' in self.resultados:
            info = self.resultados['experiment_info']
            print(f"üìä Experimento: {info.get('optimization', 'N/A')}")
            print(f"üéØ Objetivo: {info.get('objective', 'N/A')}")
            print(f"üìà Total Episodios: {info.get('total_episodes', 'N/A'):,}")
            print(f"‚è±Ô∏è  Tiempo Estimado: {info.get('estimated_time_hours', 'N/A')} horas")
        
        # An√°lisis de rendimiento
        analisis = self.analizar_rendimiento_actual()
        print(f"\nüîç AN√ÅLISIS DE RENDIMIENTO ACTUAL:")
        print(f"  ‚Ä¢ Problemas cr√≠ticos: {len(analisis['problemas_criticos'])}")
        print(f"  ‚Ä¢ Problemas moderados: {len(analisis['problemas_moderados'])}")
        print(f"  ‚Ä¢ Puntos fuertes: {len(analisis['puntos_fuertes'])}")
        
        if analisis['problemas_criticos']:
            print("\nüî¥ PROBLEMAS CR√çTICOS:")
            for problema in analisis['problemas_criticos']:
                print(f"  ‚Ä¢ {problema}")
        
        # An√°lisis de convergencia
        convergencia = self.analizar_convergencia()
        print(f"\nüìà AN√ÅLISIS DE CONVERGENCIA:")
        for key, data in convergencia.items():
            convergio = "‚úÖ" if data.get('convergio', False) else "‚ùå"
            plateando = "üîÑ" if data.get('plateando', False) else "üìà"
            print(f"  ‚Ä¢ {key}: {convergio} Mejora: {data.get('mejora_total', 0):.1f} {plateando}")
        
        # Recomendaciones
        recomendaciones = self.generar_recomendaciones_prioritarias()
        print("\n" + "\n".join(recomendaciones))
        
        # C√≥digo de mejoras
        print(f"\nüíª C√ìDIGO DE MEJORAS SUGERIDAS:")
        print(self.generar_codigo_mejoras())
        
        # Crear visualizaciones
        self.crear_visualizaciones()

def main():
    """Funci√≥n principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Analizador de resultados FLAN')
    parser.add_argument('--json', '-j', default='flan_results_196k_ultra.json',
                       help='Archivo JSON de resultados')
    parser.add_argument('--output', '-o', default='mejoras_sugeridas.py',
                       help='Archivo de salida para c√≥digo de mejoras')
    
    args = parser.parse_args()
    
    # Buscar archivos JSON disponibles
    json_files = list(Path('.').glob('flan_results*.json'))
    if not Path(args.json).exists() and json_files:
        print(f"‚ö†Ô∏è {args.json} no encontrado. Archivos disponibles:")
        for f in json_files:
            print(f"  ‚Ä¢ {f}")
        
        # Usar el m√°s reciente
        args.json = str(sorted(json_files, key=lambda x: x.stat().st_mtime)[-1])
        print(f"üìä Usando archivo m√°s reciente: {args.json}")
    
    # Ejecutar an√°lisis
    analizador = AnalizadorResultadosFLAN(args.json)
    analizador.generar_reporte_completo()
    
    # Guardar c√≥digo de mejoras
    with open(args.output, 'w') as f:
        f.write(analizador.generar_codigo_mejoras())
    
    print(f"\n‚úÖ An√°lisis completo. C√≥digo de mejoras guardado en: {args.output}")
    print(f"üìä Visualizaciones guardadas en: analisis_optimizacion_flan.png")

if __name__ == "__main__":
    main() 